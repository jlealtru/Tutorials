{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.sql import read_sql\n",
    "import os\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-02 14:42:06--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  4.06MB/s    in 22s     \n",
      "\n",
      "2019-10-02 14:42:28 (3.65 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory paths of the imdb reviews\n",
    "train_neg = 'aclImdb/train/neg'\n",
    "train_pos = 'aclImdb/train/pos'\n",
    "\n",
    "test_neg = 'aclImdb/test/neg'\n",
    "test_pos = 'aclImdb/test/pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for another implementation see https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"review\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  \n",
    "  float_dict ={'pos': 1, 'neg': 0}\n",
    "  \n",
    "  for file_path in os.listdir(directory):\n",
    "    with open(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"review\"].append(f.read())\n",
    "      \n",
    "                               \n",
    "          #re.match('.*\\/([^-]*)', file_path.group(1))\n",
    "  data[\"sentiment\"] = re.match('.*\\/([^-]*)', directory).group(1)    \n",
    "  data = pd.DataFrame.from_dict(data)\n",
    "  data[\"sentiment\"]= data[\"sentiment\"].map(float_dict)\n",
    "  return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of training data is 25,000 and test data is 25,000\n"
     ]
    }
   ],
   "source": [
    "# laod training and test data using a shuffle function\n",
    "\n",
    "training_data = pd.concat([load_directory_data(train_neg), load_directory_data(train_pos)])\n",
    "training_data = shuffle(training_data)\n",
    "\n",
    "test_data = pd.concat([load_directory_data(test_neg), load_directory_data(test_pos)])\n",
    "test_data = shuffle(test_data)\n",
    "print(f'Lenght of training data is {len(training_data):,} and test data is {len(test_data):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 2773559.70B/s]\n"
     ]
    }
   ],
   "source": [
    "# now use an iterator class that we will feed into python for training\n",
    "# Next step is define a class that takes the text and labels, tokenizes the text \n",
    "# using the bert tokenizer, converts tokens to ids, pads the sentences to make sure they are the same\n",
    "# size as the model allows; if they are longer it trims them else it pads them with 0.\n",
    "# finallly feeds themn to the classifier.\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_seq_length = 256\n",
    "\n",
    "class Data_Processing(object):\n",
    "  \n",
    "  def __init__(self, text_column, label_column):\n",
    "    \n",
    "    # define the text column from the dataframe\n",
    "    self.text_column = text_column.tolist()\n",
    "    \n",
    "    #self.label_column = pd.Categorical(pd.factorize(label_column)[0])\n",
    "    \n",
    "    # define the label column and transform it to list\n",
    "    self.label_column = label_column.tolist()\n",
    "    \n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(self.text_column[index])\n",
    "    \n",
    "    # Account for [CLS] and [SEP] with \"- 2\"\n",
    "    \n",
    "    # check for the sequence lenght taking into consideration the \n",
    "    # fact that we need to include the SEP special token and the CLS special\n",
    "    # tokens. \n",
    "    if len(tokenized_text) > max_seq_length - 2:\n",
    "      tokenized_text = tokenized_text[0:(max_seq_length - 2)]\n",
    "    \n",
    "    # We add the CLS token at the beginning of the tokenized sequence and the \n",
    "    # SEP token at the end.\n",
    "    tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "    \n",
    "    # convert the inputs to ids (dict looking)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # We define the size of the input mask to correspon to the lenght of the inputs\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    \n",
    "    \n",
    "    #attention_masks.append(seq_mask) \n",
    "\n",
    "    #input_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    \n",
    "    # \n",
    "    input_ids += padding\n",
    "    \n",
    "    #input_mask += padding\n",
    "\n",
    "    attention_masks = [1 if x>0 else 0 for x in input_ids] \n",
    "       \n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(attention_masks) == max_seq_length\n",
    "       \n",
    "    #print(ids_review)\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "        \n",
    "    labels = self.label_column[index] \n",
    "    \n",
    "    #list_of_labels = [torch.from_numpy(np.array(labels)).squeeze(0)]\n",
    "    list_of_labels = torch.tensor(labels)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    return input_ids, list_of_labels, attention_masks\n",
    "  \n",
    "  def __len__(self):\n",
    "        return len(self.text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# reference for dataloaders https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "import torch\n",
    "batch_size = 8\n",
    "\n",
    "# create a class to process the traininga and test data\n",
    "training_data = Data_Processing(training_data['review'].iloc[0:7000], training_data['sentiment'].iloc[0:7000])\n",
    "\n",
    "test_data =  Data_Processing(test_data['review'].iloc[0:7000], test_data['sentiment'].iloc[0:7000])\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=10),\n",
    "                   'val': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "                   }\n",
    "\n",
    "dataset_sizes = {'train':len(training_data),\n",
    "                'val':len(test_data)}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# Taken from Huggin Face implementation available here: \n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "    num_labels = 2\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2, output_attentions = False):\n",
    "      \n",
    "      super(BertForSequenceClassification, self).__init__()\n",
    "      #super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "\n",
    "      #super(BertForSequenceClassification, self).__init__()\n",
    "      #self.output_attentions = output_attentions\n",
    "      self.num_labels = num_labels\n",
    "      self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "      self.dropout= nn.Dropout(config.hidden_dropout_prob)\n",
    "      self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "      nn.init.xavier_normal_(self.classifier.weight)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "      \n",
    "      _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "      \n",
    "      pooled_output = self.dropout(pooled_output)\n",
    "      \n",
    "      logits = self.classifier(pooled_output)\n",
    "\n",
    "      return logits\n",
    "    \n",
    "    def freeze_bert_encoder(self):\n",
    "      for param in self.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "      \n",
    "    def unfreeze_bert_encoder(self):\n",
    "      for param in self.bert.parameters():\n",
    "        \n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:00<00:00, 171812.22B/s]\n",
      "100%|██████████| 440473133/440473133 [01:37<00:00, 4517376.73B/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "#from pytorch_transformers import BertForSequenceClassification\n",
    "#model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "config = BertConfig(num_labels=2)\n",
    "\n",
    "model = BertForSequenceClassification(config=config, num_labels=2)\n",
    "#model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score\n",
    "# define the metrics to evaluate\n",
    "def log_metrics(y_pred, y_true):\n",
    "    print('Accuracy:', accuracy_score(y_true,y_pred))\n",
    "    #print('MCC:', matthews_corrcoef(y_true,y_pred))\n",
    "    print('AUC score:', roc_auc_score(y_true, y_pred))\n",
    "    \n",
    "    # to do implement F1 score in pitchfork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "## to do \n",
    "import torch.nn.functional as F\n",
    "#import torch.nn.functional as F\n",
    "import copy \n",
    "import time\n",
    "number_steps = 2\n",
    "print(number_steps)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=3):\n",
    "\n",
    "  best_eval_acc = 0.0\n",
    "  for epoch in range(num_epochs):\n",
    "    #zero the model gradients\n",
    "    model.zero_grad()\n",
    "    print(f'starting epoch {epoch+1} out of {num_epochs}')\n",
    "    training_loss = []\n",
    "    training_accuracy = []\n",
    "    val_loss = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # track number of batches and number of iterations\n",
    "    counter = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    # Iterate over data, feeding inputs, attention masks and labels\n",
    "    model.train()\n",
    "    for i, (inputs, label, attention_mask) in enumerate(dataloaders_dict['train']):\n",
    "      # add a counter that will register how many examples we have fed to the\n",
    "      # model\n",
    "      counter+= batch_size\n",
    "      iterations+=1\n",
    "      # move the sequences, labels and masks to the GPU\n",
    "      inputs = inputs.to(device) \n",
    "      label = label.to(device)\n",
    "      attention_mask = attention_mask.to(device)\n",
    "      \n",
    "      # feed the sequences to the model, specifying the attention mask\n",
    "      outputs = model(inputs, attention_mask=attention_mask)\n",
    "      \n",
    "      # feed the logits returned by the model to the softmax to classify the function\n",
    "      outputs = F.softmax(outputs,dim=1)\n",
    "      \n",
    "      # calculate the loss function, squeeze the labels so their shapes are compatible\n",
    "      loss = criterion(outputs, label.squeeze(0))\n",
    "      \n",
    "      # divide the loss by the number of steps\n",
    "      loss_reg = loss.item() / number_steps \n",
    "      \n",
    "      #add the loss to the epoch loss\n",
    "      epoch_loss += loss_reg\n",
    "      training_loss.append(loss_reg)\n",
    "       \n",
    "      loss.backward()\n",
    "      \n",
    "      # accumulate gradients and update every 2 batches\n",
    "      if (i+1) % number_steps == 0:\n",
    "        optimizer.step()                            # Now we can do an optimizer step\n",
    "        model.zero_grad()                           # Reset gradients tensors\n",
    "      \n",
    "      # only present the information \n",
    "      if counter%800 == 0:\n",
    "        # get the predictions and the true labels out of the GPU\n",
    "        preds1 = torch.argmax(outputs,dim=1).cpu().data.numpy()\n",
    "        true1 = np.array(label.squeeze(0).cpu().data.numpy())\n",
    "      \n",
    "      # get the accurary score\n",
    "        training_accuracy.append(accuracy_score(preds1,true1))\n",
    "        print(f'current training loss is {epoch_loss/iterations} and accuracy is {np.mean(training_accuracy):,.2%}')\n",
    "        \n",
    "        #print(f'loss is {accuracy_score(preds1,true1):,.2%}')\n",
    "        #print(f'The average accuracy is {np.mean(accuracy_):,.2%} and the current loss is {loss}')\n",
    "      \n",
    "      #if epoch_loss<previous_loss:\n",
    "      #  print(f'saving the model with epoch loss {epoch_loss} of and accuracy of {np.mean(accuracy_):,.2%}')\n",
    "      #  torch.save(model.state_dict(), 'bert_imdb.pth')\n",
    "      #  torch.save(optimiser.state_dict(), 'bert_imdb_optimiser.pth')\n",
    "          \n",
    "    with torch.no_grad():\n",
    "          model.eval()\n",
    "          counter_val = 0\n",
    "          iterations_val = 0\n",
    "          for i, (inputs, label, attention_mask) in enumerate(dataloaders_dict['val']):\n",
    "            counter_val += batch_size\n",
    "            iterations_val += 1\n",
    "      \n",
    "            # move the sequences, labels and masks to the GPU\n",
    "            inputs = inputs.to(device) \n",
    "            label = label.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "          \n",
    "          # feed the sequences to the model, specifying the attention mask\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "          \n",
    "          # feed the logits returned by the model to the softmax to classify the function\n",
    "            outputs = F.softmax(outputs,dim=1)\n",
    "          \n",
    "          # calculate the loss function, squeeze the labels so their shapes are compatible\n",
    "            loss_eval = criterion(outputs, label.squeeze(0))\n",
    "            val_loss.append(loss_eval.item())\n",
    "\n",
    "            if counter_val % 800 == 0:\n",
    "              # get the predictions and the true labels out of the GPU for validation\n",
    "              preds1 = torch.argmax(outputs,dim=1).cpu().data.numpy()\n",
    "              true1 = np.array(label.squeeze(0).cpu().data.numpy())\n",
    "      \n",
    "              # get the accurary score\n",
    "              val_accuracy.append(accuracy_score(preds1,true1))\n",
    "              print(f'current validation loss is {np.sum(val_loss)/iterations_val} and accuracy is {np.mean(val_accuracy):,.2%}')\n",
    "      \n",
    "\n",
    "                        \n",
    "    print(f'For epoch {epoch+1} training loss is {np.sum(training_loss)/iterations}, training accuracy is {np.mean(training_accuracy):,.2%}, Validation loss is {np.sum(val_loss)/iterations_val} and validation accuracy is {np.mean(val_accuracy):,.2%}')\n",
    "    eval_acc = np.mean(val_accuracy)\n",
    "    if eval_acc >= best_eval_acc:\n",
    "      best_eval_acc = eval_acc\n",
    "      print(f'saving the model with validation accuracy of {eval_acc:,.2%} ')\n",
    "      torch.save(model.state_dict(), 'bert_imdb.pth')\n",
    "      torch.save(optimizer_ft.state_dict(), 'bert_imdb_optimiser.pth')\n",
    "    else:\n",
    "      print(f'model did not improve')\n",
    "      \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference this post https://towardsdatascience.com/bert-classifier-just-another-pytorch-model-881b3cf05784\n",
    "import torch.optim as optim \n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "lrlast = .001\n",
    "lrmain = .00001\n",
    "optim1 = optim.Adam(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "       \n",
    "   ])\n",
    "\n",
    "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1 out of 3\n",
      "current training loss is 0.3374501758813858 and accuracy is 50.00%\n",
      "current training loss is 0.3006599934399128 and accuracy is 68.75%\n",
      "current training loss is 0.27448181589444476 and accuracy is 75.00%\n",
      "current training loss is 0.26478370897471903 and accuracy is 78.12%\n",
      "current training loss is 0.2575100645422935 and accuracy is 80.00%\n",
      "current training loss is 0.2517114018648863 and accuracy is 79.17%\n",
      "current training loss is 0.24642134528074947 and accuracy is 80.36%\n",
      "current training loss is 0.24260049467906356 and accuracy is 82.81%\n",
      "current validation loss is 0.4007155841588974 and accuracy is 100.00%\n",
      "current validation loss is 0.41173031345009803 and accuracy is 100.00%\n",
      "current validation loss is 0.41504809727271397 and accuracy is 100.00%\n",
      "current validation loss is 0.4193042891472578 and accuracy is 93.75%\n",
      "current validation loss is 0.4190378712415695 and accuracy is 92.50%\n",
      "current validation loss is 0.4202676913142204 and accuracy is 91.67%\n",
      "current validation loss is 0.4204633069889886 and accuracy is 92.86%\n",
      "current validation loss is 0.42295276284217836 and accuracy is 93.75%\n",
      "For epoch 1 training loss is 0.2411399508374078, training accuracy is 82.81%, Validation loss is 0.42059801622799464 and validation accuracy is 93.75%\n",
      "saving the model with validation accuracy of 93.75% \n",
      "starting epoch 2 out of 3\n",
      "current training loss is 0.20854576095938682 and accuracy is 100.00%\n",
      "current training loss is 0.20458937890827655 and accuracy is 87.50%\n",
      "current training loss is 0.20526615863045056 and accuracy is 87.50%\n",
      "current training loss is 0.20726239070296287 and accuracy is 87.50%\n",
      "current training loss is 0.2075380049943924 and accuracy is 87.50%\n",
      "current training loss is 0.20691276378929616 and accuracy is 85.42%\n",
      "current training loss is 0.20723694047757557 and accuracy is 85.71%\n",
      "current training loss is 0.20741624159738423 and accuracy is 87.50%\n",
      "current validation loss is 0.4328135570883751 and accuracy is 87.50%\n",
      "current validation loss is 0.42734602630138396 and accuracy is 81.25%\n",
      "current validation loss is 0.43172429263591766 and accuracy is 87.50%\n",
      "current validation loss is 0.42548712335526945 and accuracy is 87.50%\n",
      "current validation loss is 0.42560018891096113 and accuracy is 90.00%\n",
      "current validation loss is 0.42577605103453 and accuracy is 89.58%\n",
      "current validation loss is 0.4283329913871629 and accuracy is 87.50%\n",
      "current validation loss is 0.42883206691592934 and accuracy is 89.06%\n",
      "For epoch 2 training loss is 0.20710680638040815, training accuracy is 87.50%, Validation loss is 0.4268553406170436 and validation accuracy is 89.06%\n",
      "model did not improve\n",
      "starting epoch 3 out of 3\n",
      "current training loss is 0.19196561470627785 and accuracy is 100.00%\n",
      "current training loss is 0.19070019111037254 and accuracy is 87.50%\n",
      "current training loss is 0.19366442581017812 and accuracy is 91.67%\n",
      "current training loss is 0.19500109683722258 and accuracy is 93.75%\n",
      "current training loss is 0.19684358394145965 and accuracy is 90.00%\n",
      "current training loss is 0.19681540332734584 and accuracy is 91.67%\n",
      "current training loss is 0.19721055396965573 and accuracy is 92.86%\n",
      "current training loss is 0.1965968918800354 and accuracy is 93.75%\n",
      "current validation loss is 0.4126196902990341 and accuracy is 100.00%\n",
      "current validation loss is 0.4074158550798893 and accuracy is 93.75%\n",
      "current validation loss is 0.40574767072995505 and accuracy is 91.67%\n",
      "current validation loss is 0.4079747024923563 and accuracy is 93.75%\n",
      "current validation loss is 0.4039756172299385 and accuracy is 92.50%\n",
      "current validation loss is 0.40512359485030175 and accuracy is 93.75%\n",
      "current validation loss is 0.4041593052233968 and accuracy is 94.64%\n",
      "current validation loss is 0.4047741759568453 and accuracy is 93.75%\n",
      "For epoch 3 training loss is 0.19610589463370187, training accuracy is 93.75%, Validation loss is 0.404932785987854 and validation accuracy is 93.75%\n",
      "saving the model with validation accuracy of 93.75% \n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model weights and optimizer\n",
    "model_ft1.load_state_dict(torch.load('bert_imdb.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a prediction function\n",
    "def prediction_example(text_sequence=None, model=None, max_seq_length=256, labels = True):\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(text_sequence)\n",
    "    \n",
    "    # Account for [CLS] and [SEP] with \"- 2\"\n",
    "    \n",
    "    # check for the sequence lenght taking into consideration the \n",
    "    # fact that we need to include the SEP special token and the CLS special\n",
    "    # tokens. \n",
    "    if len(tokenized_text) > max_seq_length - 2:\n",
    "        tokenized_text = tokenized_text[0:(max_seq_length - 2)]\n",
    "    \n",
    "    # We add the CLS token at the beginning of the tokenized sequence and the \n",
    "    # SEP token at the end.\n",
    "    tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "    \n",
    "    # convert the inputs to ids (dict looking)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        \n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    \n",
    "    input_ids += padding\n",
    "    \n",
    "    #input_mask += padding\n",
    "\n",
    "    attention_masks = [1 if x>0 else 0 for x in input_ids] \n",
    "       \n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(attention_masks) == max_seq_length\n",
    "       \n",
    "    #print(ids_review)\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    \n",
    "    #list_of_labels = [torch.from_numpy(np.array(labels)).squeeze(0)]\n",
    "    \n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "    # set model to eval\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = input_ids.to(device) \n",
    "    attention_mask = attention_masks.to(device)\n",
    "    \n",
    "    prediction = model_ft1(inputs.unsqueeze(0), attention_mask=attention_mask)\n",
    "    result = torch.argmax(prediction,dim=1).cpu().data.numpy()\n",
    "    \n",
    "    label_values = {\"Positive\": 1, \"Negative\": 0}\n",
    "\n",
    "    if labels:\n",
    "        for key, value in label_values.items():\n",
    "            if result == value:\n",
    "                print (key)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "text_to_predict = prediction_example(text_sequence = 'One of the greatests movies in the spagethi western genre. \\\n",
    "Wonderful performances', model = model_ft1, max_seq_length = 256, labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-3bbe462a9880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabel_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Positive\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Negative\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
