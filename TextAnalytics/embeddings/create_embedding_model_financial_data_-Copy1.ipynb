{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the relevant libraries for our analysis\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify device type\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial serves as an introduction to the usage of torchtext a library designed to preprocess text data so it\n",
    "can be usedin tamdem with pytorch as well as with other deep learning libraries\n",
    "\n",
    "For this tutorial we will use the Consumer Complaint Database put together by the Consumer Financial \n",
    "Protection Bureau (CBPB). More info about the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic text preprocessing\n",
    "\n",
    "We will leverage Spacy's modern text preprocessing methods to lemmatize, handle some spelling errors, create a pronoun flag, etc. The first step in our code is to load the large english language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy is one of the most powerful NLP libraries available for data processing (a word on why spacy is better?)/ The next step is to define the data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "#nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Consumer_Complaints.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default data dir\n",
    "basepath = '/media/jlealtru/data_files/github/Tutorials'\n",
    "\n",
    "DATA_DIR = '/media/jlealtru/data_files/github/Tutorials/datasets/text_analytics/financial'\n",
    "\n",
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is quite big, we are not hosting on github, you can get it from here:\n",
    "link    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the complains data to have a better idea on how this works\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, 'Consumer_Complaints.csv'), engine='python')\n",
    "print(f\"The dataframe has {len(df):,} observations.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in predicting the type of issue faced by the customer. In the dataset we have 18 different\n",
    "types of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Product','Complaint ID']].groupby('Product').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Despite the fact the data has more than 1 million observations, a coursory exploration reveals that the data \n",
    "has multiple missing values. Checking the actual distribution of answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Consumer consent provided?','Complaint ID']].groupby('Consumer consent provided?').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have complete information for {len(df[df['Consumer consent provided?']=='Consent provided']):,} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the data to only have observations with consumer narrative, select relevant columns and shuffle \n",
    "the data. We use the pandas native sample function that generates a random sample of lenght n, in this \n",
    "case it is  equal to the number of the observations on the filtered dataset. We also set the random stated\n",
    "to a predifined number so we can replicate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['Consumer consent provided?']=='Consent provided']\n",
    "df=df[['Complaint ID','Consumer complaint narrative','Product']]\n",
    "df=df.sample(n=df.shape[0], random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of cases for product\n",
    "df[['Complaint ID', 'Product']].groupby('Product').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that we have twp classes that are very unbalanced, Virtual Currency and Other financial Services. \n",
    "# To improve the performance of the model we merge both into a single class\n",
    "df['Product'][(df['Product']=='Virtual currency') | (df['Product']=='Other financial service')]='Other financial services'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if this works\n",
    "df[['Complaint ID', 'Product']].groupby('Product').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset the index and rename the text containing the text field of our data, we will use\n",
    "that text in the torchtext context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Consumer complaint narrative': 'text'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values_target={k:v for k,v in zip(range(len(df.Product.unique())), df.Product.unique())}\n",
    "product_lookup=pd.DataFrame([[key,value] for key,value in dict_values_target.items()],\n",
    "     columns=[\"product_id\",\"product_text\"])\n",
    "product_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df=pd.merge(df,product_lookup, how='left', left_on='Product',right_on='product_text')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['text', 'product_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next we create, validation, training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test, train and validation datasets \n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "test_= df[~msk]\n",
    "msk1=np.random.rand(len(test_)) <= 0.5\n",
    "test=test_[msk1]\n",
    "val=test_[~msk1]\n",
    "del test_\n",
    "#df.iloc[0:700000].to_csv(os.path.join(basepath, 'train.csv'), index=False)\n",
    "#df.iloc[700000:800000].to_csv(os.path.join(basepath, 'test.csv'), index=False)#\n",
    "#df.iloc[800000:900000].to_csv(os.path.join(basepath, 'valid.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The len of train dataset is {len(train):,}, the len of test is {len(test):,} and the len of valid is \"+\n",
    "     f\"{len(val):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "#Complaint ID Product\n",
    "train.to_csv(os.path.join(basepath, 'train.csv'),index=False)\n",
    "test.to_csv(os.path.join(basepath, 'test.csv'),index=False)\n",
    "val.to_csv(os.path.join(basepath, 'val.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv(os.path.join(basepath, 'test.csv'))\n",
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the tokenizer\n",
    "tokenize_count = 0\n",
    "\n",
    "# use custom tokenizer with large spacy model\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    global tokenize_count\n",
    "    if tokenize_count % 1000 == 0:\n",
    "        sys.stdout.write('\\rDoc: {}'.format(tokenize_count))\n",
    "        sys.stdout.flush()\n",
    "    tokenize_count += 1\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=tokenizer)\n",
    "#LABEL = data.Field(sequential=False, \n",
    "#                   use_vocab=False, \n",
    "#                   pad_token=None, \n",
    "#                   unk_token=None)\n",
    "#LABEL=data.LabelField(dtype=torch.float)\n",
    "#LABEL = data.LabelField(dtype=torch.float)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_datafields = [  ('text', TEXT), ('product_id', LABEL)\n",
    "               # we won't be needing the id, so we pass in None as the field\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "from torchtext import data\n",
    "\n",
    "# define \n",
    "SEED = 1234\n",
    "\n",
    "#  add a line about cudnn feterministic\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: 277000"
     ]
    }
   ],
   "source": [
    "# Use the tabular dataset split function to create\n",
    "\n",
    "trn, vld, tst = TabularDataset.splits(\n",
    "    path=\"data\", # the root directory where the data lies\n",
    "    train=os.path.join(basepath, 'train.csv'), \n",
    "    validation=os.path.join(basepath, 'val.csv'),\n",
    "    test=os.path.join(basepath, 'test.csv'), \n",
    "    format='csv',\n",
    "    skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "    fields=_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the process worked fine\n",
    "trn.fields.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary using glove vectors of 300 dimensions. To limit the size of the vocabulary we limit the \n",
    "# vocabulary to 50,000 in size and a minumun occurence of 5 times\n",
    "TEXT.build_vocab(trn, vectors='glove.42B.300d', min_freq=5, \n",
    "                max_size=50000)\n",
    "LABEL.build_vocab(trn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of unique tokens and the len of the label categories\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn[0].product_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the most frequent words in the vocabulary\n",
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our batch iterator object for training. This will automatically \n",
    "# shift our input text forward t+1 for our target data for the language model \n",
    "# to predict the next word in the sequence\n",
    "train_iter, test_iter, valid_iter = data.BucketIterator.splits(\n",
    "    (trn, tst, vld), \n",
    "    batch_size=32,\n",
    "    #bptt_len=35, # specifying the sequence length for back prop through time\n",
    "    device=device,\n",
    "    #repeat=False, \n",
    "    sort_key=lambda x: len(x.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data split getting an observation from the training iterable\n",
    "b=next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.product_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a LSTM RNN, a pretty standard model used to classify sequential data. For a refresher of RNN you can check out the good videos of Deep AI (they tend to be a bit theory heavy). The deep learning for coders is another great resource if you are interested in videos of Fastai Some of the most common tasks in pytorch is the classif(link to discussion on where.\n",
    "\n",
    "The models from torch are saved on the nn module. We define a class that will hold the model, layers and parameters necessary for our sample. You may notice the use of the super parameter (Jason wanna chime in here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the LSTM model \n",
    "  - add discussion of the model\n",
    "  - add reference to the LSTM model and a link to a couple of tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class that will hold the model as well as the necessary parameters for it to work\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters of the model:\n",
    "        vocab_size: size of the vocabulary after creating it using the Glove embeddings with the defined thresholds\n",
    "        embedding_dim: size of the vocabulary embeddings\n",
    "        hidden_dim: hidden dimensions of the lstm model\n",
    "        output_dim: number of classes in our data\n",
    "        n_layers: number of lstm layers\n",
    "        bidirectional: dummy to specify if this is a bidirectional model\n",
    "        dropout: droptout parameter for the dropout model\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        # return the hidden state and the cell. We will concatenate the last two hidden vectors\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        # apply dropout before passing to fully connected layer\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        #define output\n",
    "        output=self.fc(hidden)\n",
    "        \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters for the model and instatiate the model class\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pretrained_embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the embedding weights to the model and move the model to the gpu\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss and optimizer\n",
    "#loss_function = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "#nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.optim as optim\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# define our loss function and the parameters for updating the model.\n",
    "# TALK ABOUT BETAS\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.7, 0.99))\n",
    "n_tokens = pretrained_embeddings.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(nn.CrossEntropyLoss)\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def accuracy(out, labels):\n",
    "#    return torch.sum(labels.data == out)/float(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_i, batch in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch.text = batch.text.cuda()\n",
    "        batch.product_id=batch.product_id.cuda()\n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "        \n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "        #loss = criterion(predictions, batch.product_id)\n",
    "        #if predictions[0].shape==batch.product_id.shape[0]:\n",
    "            \n",
    "    \n",
    "        loss=criterion(predictions, batch.product_id)\n",
    "        \n",
    "        \n",
    "        #acc = binary_accuracy(predictions, batch.product_id)\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        correct = (torch.max(predictions, 1)[1] == batch.product_id).sum()\n",
    "        #print(float(correct)/32)\n",
    "        if batch_i % 10 == 9:    # print every 10 batches\n",
    "            print('Epoch: {}, Batch: {}, Avg. Loss: {}, correct{}'.format(epoch + 1,\n",
    "                                                                          batch_i+1, running_loss/1000,\n",
    "                                                                          float(correct)/32))        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(train_iter)))\n",
    "        #epoch_loss += loss.item()\n",
    "        \n",
    "        #correct += (predicted.type(torch.DoubleTensor) == labels).sum()\n",
    "        #pred=predictions[0]\n",
    "        #print(torch.max(predictions, 0)[1],batch.product_id)\n",
    "        #print(torch.max(predictions, 1)[1].shape)\n",
    "        #correct = (rounded_preds == y).float()\n",
    "        \n",
    "        #correct = (output == batch.product_id).float().sum()\n",
    "        #correct/batch.product_id.shape[0]\n",
    "        #epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test function to measure accuracy\n",
    "\n",
    "def test_data():\n",
    "    \n",
    "    # iterate through the test dataset\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        \n",
    "        # move inputs to gpu\n",
    "        batch.text = batch.text.cuda()\n",
    "        batch.product_id=batch.product_id.cuda()\n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "        \n",
    "        loss=criterion(predictions, batch.product_id)\n",
    "                \n",
    "        #acc = binary_accuracy(predictions, batch.product_id)\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        correct = (torch.max(predictions, 1)[1] == batch.product_id).sum()\n",
    "        #print(float(correct)/32)\n",
    "        if batch_i % 10 == 9:    # print every 10 batches\n",
    "            print('Epoch: {}, Batch: {}, Avg. Loss: {}, correct{}'.format(epoch + 1,\n",
    "                                                                          batch_i+1, running_loss/1000,\n",
    "                                                                          float(correct)/32))        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    #train_loss, train_acc = \n",
    "    train(model, train_iter, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.functional.softmax(pr1[0,:])\n",
    "print(b1)\n",
    "print(torch.max(torch.nn.functional.softmax(pr1), 1)[1])\n",
    "#(np.log(pr1[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loss(valid_iter, model):\n",
    "    \n",
    "    # monitor the loss\n",
    "    val_loss = 0\n",
    "    # turn on evaluation mode\n",
    "    model.eval()\n",
    "    for batch in valid_iter:\n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        val_loss += loss.item() * text.size(0)\n",
    "    val_loss /= len(valid.examples[0].text)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        \n",
    "        val_loss += loss.item() * text.size(0)\n",
    "        \n",
    "        acc = validation_loss(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets.language_modeling import LanguageModelingDataset\n",
    "\n",
    "\n",
    "\n",
    "class CustomLMData(LanguageModelingDataset):\n",
    "    \n",
    "    name = 'lm_dataset'\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, _datafields, root=None, train='lmdata.txt',\n",
    "               validation=None, test=None,\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Create dataset from custom data persisted to disc. Data\n",
    "        must be newline separated text files and path must be designated. \n",
    "        \n",
    "        Arguments:\n",
    "            text_field: The field that will be used for text data.\n",
    "            root: The root directory that the dataset's zip archive will be\n",
    "                expanded into; therefore the directory in whose wikitext-2\n",
    "                subdirectory the data files will be stored.\n",
    "            train: The filename of the train data. Default: 'wiki.train.tokens'.\n",
    "            validation: The filename of the validation data, or None to not\n",
    "                load the validation set. Default: 'wiki.valid.tokens'.\n",
    "            test: The filename of the test data, or None to not load the test\n",
    "                set. Default: 'wiki.test.tokens'.\n",
    "                \n",
    "        Resources: \n",
    "            https://github.com/pytorch/text/blob/master/torchtext/data/dataset.py\n",
    "            https://github.com/pytorch/text/blob/master/torchtext/datasets/language_modeling.py\n",
    "            https://torchtext.readthedocs.io/en/latest/examples.html\n",
    "        \"\"\"\n",
    "        return super(CustomLMData, cls).splits(\n",
    "            root=root, train=train, validation=validation, test=test,\n",
    "            fields=_datafields, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def iters(cls, batch_size=32, bptt_len=25, device=None, path=basepath,\n",
    "              train='lmdata.txt', validation=None, test=None, root=basepath,\n",
    "              vectors=None, **kwargs):\n",
    "        \"\"\"Create iterator objects for splits of the WikiText-2 dataset.\n",
    "        This is the simplest way to use the dataset, and assumes common\n",
    "        defaults for field, vocabulary, and iterator parameters.\n",
    "        Arguments:\n",
    "            batch_size: Batch size.\n",
    "            bptt_len: Length of sequences for backpropagation through time.\n",
    "            device: Device to create batches on. Use -1 for CPU and None for\n",
    "                the currently active GPU device.\n",
    "            root: The root directory that the dataset's zip archive will be\n",
    "                expanded into; therefore the directory in whose wikitext-2\n",
    "                subdirectory the data files will be stored.\n",
    "            wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n",
    "                text field. The word vectors are accessible as\n",
    "                train.dataset.fields['text'].vocab.vectors.\n",
    "            Remaining keyword arguments: Passed to the splits method.\n",
    "        \"\"\"\n",
    "        TEXT = data.Field()\n",
    "\n",
    "        train = cls.splits(TEXT, root=root, path=basepath, **kwargs)\n",
    "\n",
    "        TEXT.build_vocab(train, vectors=vectors)\n",
    "\n",
    "        return data.BPTTIterator.splits(train,\n",
    "            batch_size=batch_size, bptt_len=bptt_len,\n",
    "            device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, valid = CustomLMData.splits(\n",
    "    TEXT,\n",
    "    path=basepath,\n",
    "    train='train.csv',\n",
    "    test='test.csv',\n",
    "    validation='valid.csv',\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors='glove.42B.300d', min_freq=5, \n",
    "                max_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, valid = CustomLMData.splits(\n",
    "    TEXT,\n",
    "    path=basepath,\n",
    "    train='train.csv',\n",
    "    test='test.csv',\n",
    "    validation='valid.csv',\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We now need to write the customer complaints to a file separated by new lines so we can use it in the\n",
    "torch model. # for the language modelling, we are going to write out to a text file\n",
    "# that is new line separated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def write_text_data(file_name,text_field):\n",
    "    text\n",
    "    with open(os.path.join(basepath, fname), 'w',newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter = '\\n')\n",
    "        writer.writerow(text_field)\n",
    "\n",
    "write_text_data('train2.csv', train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train['text'])\n",
    "train.to_csv(os.path.join(basepath, 'train.csv'), index=False)\n",
    "len(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for line in open(os.path.join(basepath,'train.csv')))#train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for line in open(os.path.join(basepath,'train.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_data(fname, text):\n",
    "\n",
    "    with open(os.path.join(basepath, fname), 'w') as outfile:\n",
    "        for line in text:\n",
    "            outfile.write('\\n{}'.format(line))\n",
    "        outfile.close()\n",
    "        \n",
    "write_text_data('train.csv', train['text'])\n",
    "write_text_data('test.csv', test['text'])\n",
    "write_text_data('valid.csv', val['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the example of machine learning explained defined our tokenizer using spacy. The main purpose of this funtion if to output to screen the evolution of the training document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_count = 0\n",
    "\n",
    "# use custom tokenizer with large spacy model\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    global tokenize_count\n",
    "    if tokenize_count % 1000 == 0:\n",
    "        sys.stdout.write('\\rDoc: {}'.format(tokenize_count))\n",
    "        sys.stdout.flush()\n",
    "    tokenize_count += 1\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the text\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "#  add a line about cudnn feterministic\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors='glove.42B.300d', min_freq=5, \n",
    "                max_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_datafields = [(\"Complaint ID\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                  (\"text\", TEXT),\n",
    "                 (\"Product\", TEXT)]\n",
    "tst = TabularDataset(\n",
    "           path=basepath, # the file path\n",
    "           format='txt',\n",
    "           skip_header=False, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "           fields=tst_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We now define a tokenizer function that will split each one of the files into the tokens we will use\n",
    "for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_count = 0\n",
    "\n",
    "# use custom tokenizer with large spacy model\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    global tokenize_count\n",
    "    if tokenize_count % 1000 == 0:\n",
    "        sys.stdout.write('\\rDoc: {}'.format(tokenize_count))\n",
    "        sys.stdout.flush()\n",
    "    tokenize_count += 1\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchtext has a series of classes to handle text data. The data.field function is the way that torchtext handles our text data. In our case we are transforming words into numeric representations so we set the paramerter sequential to True. If your data passes a numericalized field and is not sequential, you should pass use_vocab=False and sequential=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "#  add a line about cudnn feterministic\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(1 for line in open(os.path.join(basepath,'train.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We need to talk about the splits function from torhectext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets.language_modeling import LanguageModelingDataset\n",
    "\n",
    "\n",
    "\n",
    "class CustomLMData(LanguageModelingDataset):\n",
    "    \n",
    "    name = 'lm_dataset'\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, root=None, train='lmdata.txt',\n",
    "               validation=None, test=None,\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Create dataset from custom data persisted to disc. Data\n",
    "        must be newline separated text files and path must be designated. \n",
    "        \n",
    "        Arguments:\n",
    "            text_field: The field that will be used for text data.\n",
    "            root: The root directory that the dataset's zip archive will be\n",
    "                expanded into; therefore the directory in whose wikitext-2\n",
    "                subdirectory the data files will be stored.\n",
    "            train: The filename of the train data. Default: 'wiki.train.tokens'.\n",
    "            validation: The filename of the validation data, or None to not\n",
    "                load the validation set. Default: 'wiki.valid.tokens'.\n",
    "            test: The filename of the test data, or None to not load the test\n",
    "                set. Default: 'wiki.test.tokens'.\n",
    "                \n",
    "        Resources: \n",
    "            https://github.com/pytorch/text/blob/master/torchtext/data/dataset.py\n",
    "            https://github.com/pytorch/text/blob/master/torchtext/datasets/language_modeling.py\n",
    "            https://torchtext.readthedocs.io/en/latest/examples.html\n",
    "        \"\"\"\n",
    "        return super(CustomLMData, cls).splits(\n",
    "            root=root, train=train, validation=validation, test=test,\n",
    "            fields=_, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def iters(cls, batch_size=32, bptt_len=25, device=0, path=basepath,\n",
    "              train='lmdata.txt', validation=None, test=None, root=basepath,\n",
    "              vectors=None, **kwargs):\n",
    "        \"\"\"Create iterator objects for splits of the WikiText-2 dataset.\n",
    "        This is the simplest way to use the dataset, and assumes common\n",
    "        defaults for field, vocabulary, and iterator parameters.\n",
    "        Arguments:\n",
    "            batch_size: Batch size.\n",
    "            bptt_len: Length of sequences for backpropagation through time.\n",
    "            device: Device to create batches on. Use -1 for CPU and None for\n",
    "                the currently active GPU device.\n",
    "            root: The root directory that the dataset's zip archive will be\n",
    "                expanded into; therefore the directory in whose wikitext-2\n",
    "                subdirectory the data files will be stored.\n",
    "            wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n",
    "                text field. The word vectors are accessible as\n",
    "                train.dataset.fields['text'].vocab.vectors.\n",
    "            Remaining keyword arguments: Passed to the splits method.\n",
    "        \"\"\"\n",
    "        TEXT = data.Field()\n",
    "\n",
    "        train = cls.splits(TEXT, root=root, path=basepath, **kwargs)\n",
    "\n",
    "        TEXT.build_vocab(train, vectors=vectors)\n",
    "\n",
    "        return data.BPTTIterator.splits(train,\n",
    "            batch_size=batch_size, bptt_len=bptt_len,\n",
    "            device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using our custom dataset class that inherits from the languagemodelling dataset of \n",
    "# torchtext, create our train, test, valid splits of quora questions\n",
    "train, test, valid = CustomLMData.splits(\n",
    "    TEXT,\n",
    "    path=basepath,\n",
    "    train='train.txt',\n",
    "    test='test.txt',\n",
    "    validation='valid.txt',\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "basepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "# to see available pretrained embedding options, take a peek at the source code:\n",
    "# https://github.com/pytorch/text/blob/master/torchtext/vocab.py\n",
    "TEXT.build_vocab(train, vectors='glove.42B.300d', min_freq=5, \n",
    "                max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our batch iterator object for training. This will automatically \n",
    "# shift our input text forward t+1 for our target data for the language model \n",
    "# to predict the next word in the sequence\n",
    "train_iter, test_iter, valid_iter = data.BPTTIterator.splits(\n",
    "    (train, test, valid), \n",
    "    batch_size=64, \n",
    "    bptt_len=25, # specifying the sequence length for back prop through time\n",
    "    device=device,\n",
    "    repeat=False, \n",
    "    sort_key=lambda x: len(x.text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerilization occurs\n",
    "b.text[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can peep into the numerilization with\n",
    "TEXT.vocab.itos[1656]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.target[:5, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and Training the language model\n",
    "\n",
    "The goal here is to use the pretrained glove 300 dimensional vectors to hot start our embedding model that will be fine tuned on our actual data. We are going to build an RNN bidirectional language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "   \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    " \n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, ntoken, ninp,\n",
    "                 nhid, nlayers, bsz,\n",
    "                 dropout=0.5, tie_weights=True):\n",
    "        \"\"\"\n",
    "        Bidirectional language model \n",
    "        \n",
    "        https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py\n",
    "        \"\"\"\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.bsz = bsz\n",
    "        self.tie_weights = tie_weights # TODO: figure out tying weight with bidirectional LSTM\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout, bidirectional=True)\n",
    "        self.decoder = nn.Linear(nhid*2, ntoken) # we need *2 for bidirectional\n",
    "        self.init_weights()\n",
    "        self.hidden = self.init_hidden(bsz) # the input is a batched consecutive corpus\n",
    "                                            # therefore, we retain the hidden state across batches\n",
    "     \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    " \n",
    "    def forward(self, input_data):\n",
    "        emb = self.drop(self.encoder(input_data))\n",
    "        output, self.hidden = self.lstm(emb, self.hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
    " \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        # once again we need x2 for bidirectional LSTM\n",
    "        return (V(weight.new(self.nlayers*2, bsz, self.nhid).zero_().cuda()),\n",
    "                V(weight.new(self.nlayers*2, bsz, self.nhid).zero_()).cuda())\n",
    "  \n",
    "    def reset_history(self):\n",
    "        self.hidden = tuple(V(v.data) for v in self.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to use our pretrained embeddings to init the RNN\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "weight_matrix = TEXT.vocab.vectors\n",
    "model = BiRNN(weight_matrix.size(0), \n",
    "                 weight_matrix.size(1), 200, 4, BATCH_SIZE, \n",
    "             tie_weights=True)\n",
    "\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "# define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.7, 0.99))\n",
    "n_tokens = weight_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the evaluation criteria\n",
    "\n",
    "def validation_loss(valid_iter, model):\n",
    "    \n",
    "    # monitor the loss\n",
    "    val_loss = 0\n",
    "    # turn on evaluation mode\n",
    "    model.eval()\n",
    "    for batch in valid_iter:\n",
    "        model.reset_history()\n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        val_loss += loss.item() * text.size(0)\n",
    "    val_loss /= len(valid.examples[0].text)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/\n",
    "\n",
    "# and write our training loop\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def clip_grads(model, clip_weight=0.25):\n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_weight)\n",
    "    for p in model.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "    \n",
    "\n",
    "def train_model(num_epochs=10):\n",
    "    \"\"\"One epoch of a training loop\"\"\"\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "        # turn on training mode\n",
    "        epoch_loss = 0\n",
    "        t = tqdm(train_iter)\n",
    "        batch_ii = 0\n",
    "        for batch in t:\n",
    "            batch_ii += 1\n",
    "            # reset the hidden state or else the model will try to backpropagate to the\n",
    "            # beginning of the dataset, requiring lots of time and a lot of memory\n",
    "            model.train()\n",
    "            t.set_description('Epoch: {}'.format(epoch))\n",
    "            t.refresh()\n",
    "            model.reset_history()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text, targets = batch.text, batch.target\n",
    "            prediction = model(text)\n",
    "            # pytorch currently only supports cross entropy loss for inputs of 2 or 4 dimensions.\n",
    "            # we therefore flatten the predictions out across the batch axis so that it becomes\n",
    "            # shape (batch_size * sequence_length, n_tokens)\n",
    "            # in accordance to this, we reshape the targets to be\n",
    "            # shape (batch_size * sequence_length)\n",
    "            loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip gradients\n",
    "            clip_grads(model)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # epoch_loss += loss.data[0] * prediction.size(0) * prediction.size(1)\n",
    "            epoch_loss += loss.item() * prediction.size(0) * prediction.size(1)\n",
    "\n",
    "            epoch_loss /= len(train.examples[0].text)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # print('Epoch: {}, Training Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "        # capture validation loss for each batch\n",
    "        valid_loss = validation_loss(valid_iter, model)\n",
    "        print('Epoch: {} | Training Loss: {:.4f} | Valid Loss: {:.4f}'.format(epoch, \n",
    "                                                                             epoch_loss, \n",
    "                                                                             valid_loss))\n",
    " \n",
    "    final_val_loss = validation_loss(valid_iter, model)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, \n",
    "                                                                             epoch_loss, \n",
    "                                                                             final_val_loss))\n",
    "\n",
    "    \n",
    "train_model(num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "if not os.path.exists(os.path.join(basepath, 'models')):\n",
    "    os.makedirs(os.path.join(basepath, 'models'))\n",
    "    \n",
    "# save entire model - if only wanting to save for inference, \n",
    "# use model.state_dict()\n",
    "torch.save(model, os.path.join(basepath, 'models/lm_200_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.load(os.path.join(basepath, 'models/lm_200_model.pt'))\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ids_to_sentence(id_tensor, vocab, join=None):\n",
    "    \"\"\"Converts a sequence of word ids to a sentence\"\"\"\n",
    "    if isinstance(id_tensor, torch.LongTensor):\n",
    "        ids = id_tensor.transpose(0, 1).contiguous().view(-1)\n",
    "    elif isinstance(id_tensor, np.ndarray):\n",
    "        ids = id_tensor.transpose().reshape(-1)\n",
    "    batch = [vocab.itos[ind] for ind in ids] # denumericalize\n",
    "    if join is None:\n",
    "        return batch\n",
    "    else:\n",
    "        return join.join(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "arrs = model(b.text).cpu().data.numpy()\n",
    "word_ids_to_sentence(np.argmax(arrs, axis=2), TEXT.vocab, join=' ')[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = TEXT.vocab\n",
    "\n",
    "vocab.stoi['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab.__dict__['freqs'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out vocab items\n",
    "\n",
    "wrd_to_embedding = {}\n",
    "for wrd in list(vocab.__dict__['freqs'].keys()):\n",
    "    print(wrd)\n",
    "    lookup_tensor = torch.tensor([vocab.stoi[wrd]], dtype=torch.long, device=device)\n",
    "    emb = model.drop(model.encoder(lookup_tensor))\n",
    "    # convert embedding to numpy array\n",
    "    emb = emb.cpu()\n",
    "    np_array = emb.detach().numpy()\n",
    "    wrd_to_embedding[wrd] = np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "cosine(wrd_to_embedding['successful'], wrd_to_embedding['pick'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(wrd_to_embedding['soldier'], wrd_to_embedding['war'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if vectors actually drifted by looking at our original vectors \n",
    "# from the glove implementation\n",
    "old_w2v = {}\n",
    "\n",
    "for wrd in list(vocab.__dict__['freqs'].keys()):\n",
    "    print(wrd)\n",
    "    wrd_id = vocab.stoi[wrd]\n",
    "    vocab.vectors[wrd_id].cpu().detach().numpy()\n",
    "    old_w2v[wrd] = np_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(wrd_to_embedding['war'], old_w2v['war'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
