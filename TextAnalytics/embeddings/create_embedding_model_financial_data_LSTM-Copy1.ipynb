{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the relevant libraries for our analysis\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify device type\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial serves as an introduction to the usage of torchtext a library designed to preprocess text data so it\n",
    "can be usedin tamdem with pytorch as well as with other deep learning libraries\n",
    "\n",
    "For this tutorial we will use the Consumer Complaint Database put together by the Consumer Financial \n",
    "Protection Bureau (CBPB). More info about the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic text preprocessing\n",
    "\n",
    "We will leverage Spacy's modern text preprocessing methods to lemmatize, handle some spelling errors, create a pronoun flag, etc. The first step in our code is to load the large english language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy is one of the most powerful NLP libraries available for data processing (a word on why spacy is better?)/ The next step is to define the data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "#nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default data dir\n",
    "basepath = '/media/jlealtru/data_files/github/Tutorials'\n",
    "\n",
    "DATA_DIR = '/media/jlealtru/data_files/github/Tutorials/datasets/text_analytics/financial'\n",
    "\n",
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is quite big, we are not hosting on github, you can get it from here:\n",
    "link    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the complains data to have a better idea on how this works\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, 'Consumer_Complaints.csv'), engine='python')\n",
    "print(f\"The dataframe has {len(df):,} observations.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in predicting the type of issue faced by the customer. In the dataset we have 18 different\n",
    "types of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Product','Complaint ID']].groupby('Product').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Despite the fact the data has more than 1 million observations, a coursory exploration reveals that the data \n",
    "has multiple missing values. Checking the actual distribution of answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Consumer consent provided?','Complaint ID']].groupby('Consumer consent provided?').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have complete information for {len(df[df['Consumer consent provided?']=='Consent provided']):,} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the data to only have observations with consumer narrative, select relevant columns and shuffle \n",
    "the data. We use the pandas native sample function that generates a random sample of lenght n, in this \n",
    "case it is  equal to the number of the observations on the filtered dataset. We also set the random stated\n",
    "to a predifined number so we can replicate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df['Consumer consent provided?']=='Consent provided']\n",
    "df=df[['Complaint ID','Consumer complaint narrative','Product']]\n",
    "df=df.sample(n=df.shape[0], random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of cases for product\n",
    "df[['Complaint ID', 'Product']].groupby('Product').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that we have twp classes that are very unbalanced, Virtual Currency and Other financial Services. \n",
    "# To improve the performance of the model we merge both into a single class\n",
    "df['Product'][(df['Product']=='Virtual currency') | (df['Product']=='Other financial service')]='Other financial services'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if this works\n",
    "df[['Complaint ID', 'Product']].groupby('Product').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reset the index and rename the text containing the text field of our data, we will use\n",
    "that text in the torchtext context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Consumer complaint narrative': 'text'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_values_target={k:v for k,v in zip(range(len(df.Product.unique())), df.Product.unique())}\n",
    "product_lookup=pd.DataFrame([[key,value] for key,value in dict_values_target.items()],\n",
    "     columns=[\"product_id\",\"product_text\"])\n",
    "product_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df=pd.merge(df,product_lookup, how='left', left_on='Product',right_on='product_text')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['text', 'product_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next we create, validation, training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test, train and validation datasets \n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "test_= df[~msk]\n",
    "msk1=np.random.rand(len(test_)) <= 0.5\n",
    "test=test_[msk1]\n",
    "val=test_[~msk1]\n",
    "del test_\n",
    "#df.iloc[0:700000].to_csv(os.path.join(basepath, 'train.csv'), index=False)\n",
    "#df.iloc[700000:800000].to_csv(os.path.join(basepath, 'test.csv'), index=False)#\n",
    "#df.iloc[800000:900000].to_csv(os.path.join(basepath, 'valid.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The len of train dataset is {len(train):,}, the len of test is {len(test):,} and the len of valid is \"+\n",
    "     f\"{len(val):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "#Complaint ID Product\n",
    "train.to_csv(os.path.join(basepath, 'train.csv'),index=False)\n",
    "test.to_csv(os.path.join(basepath, 'test.csv'),index=False)\n",
    "val.to_csv(os.path.join(basepath, 'val.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv(os.path.join(basepath, 'test.csv'))\n",
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the tokenizer\n",
    "tokenize_count = 0\n",
    "\n",
    "# use custom tokenizer with large spacy model\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    global tokenize_count\n",
    "    if tokenize_count % 1000 == 0:\n",
    "        sys.stdout.write('\\rDoc: {}'.format(tokenize_count))\n",
    "        sys.stdout.flush()\n",
    "    tokenize_count += 1\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=tokenizer)\n",
    "#LABEL = data.Field(sequential=False, \n",
    "#                   use_vocab=False, \n",
    "#                   pad_token=None, \n",
    "#                   unk_token=None)\n",
    "#LABEL=data.LabelField(dtype=torch.float)\n",
    "#LABEL = data.LabelField(dtype=torch.float)\n",
    "LABEL = data.LabelField(sequential=False, use_vocab=False, pad_token=None, \n",
    "                        unk_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_datafields = [  ('text', TEXT), ('product_id', LABEL)\n",
    "               # we won't be needing the id, so we pass in None as the field\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "from torchtext import data\n",
    "\n",
    "# define \n",
    "SEED = 1234\n",
    "\n",
    "#  add a line about cudnn feterministic\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tabular dataset split function to create\n",
    "\n",
    "trn, vld, tst = TabularDataset.splits(\n",
    "    path=\"data\", # the root directory where the data lies\n",
    "    train=os.path.join(basepath, 'train.csv'), \n",
    "    validation=os.path.join(basepath, 'val.csv'),\n",
    "    test=os.path.join(basepath, 'test.csv'), \n",
    "    format='csv',\n",
    "    skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "    fields=_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the process worked fine\n",
    "trn.fields.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary using glove vectors of 300 dimensions. To limit the size of the vocabulary we limit the \n",
    "# vocabulary to 50,000 in size and a minumun occurence of 5 times\n",
    "TEXT.build_vocab(trn, vectors='glove.42B.300d', min_freq=5, \n",
    "                max_size=50000)\n",
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of unique tokens and the len of the label categories\n",
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=[1/value for key, value in LABEL.vocab.freqs.items()]\n",
    "#print(dict(LABEL.vocab.freqs))\n",
    "\n",
    "#k,v for dict(LABEL.vocab.freqs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the most frequent words in the vocabulary\n",
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our batch iterator object for training. This will automatically \n",
    "# shift our input text forward t+1 for our target data for the language model \n",
    "# to predict the next word in the sequence\n",
    "train_iter, test_iter, valid_iter = data.BucketIterator.splits(\n",
    "    (trn, tst, vld), \n",
    "    batch_size=32,\n",
    "    #bptt_len=35, # specifying the sequence length for back prop through time\n",
    "    device=device,\n",
    "    #repeat=False, \n",
    "    sort_key=lambda x: len(x.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data split getting an observation from the training iterable\n",
    "b=next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.product_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a LSTM RNN, a pretty standard model used to classify sequential data. For a refresher of RNN you can check out the good videos of Deep AI (they tend to be a bit theory heavy). The deep learning for coders is another great resource if you are interested in videos of Fastai Some of the most common tasks in pytorch is the classif(link to discussion on where.\n",
    "\n",
    "The models from torch are saved on the nn module. We define a class that will hold the model, layers and parameters necessary for our sample. You may notice the use of the super parameter (Jason wanna chime in here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the LSTM model \n",
    "  - add discussion of the model\n",
    "  - add reference to the LSTM model and a link to a couple of tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a class that will hold the model as well as the necessary parameters for it to work\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Parameters of the model:\n",
    "        vocab_size: size of the vocabulary after creating it using the Glove embeddings with the defined thresholds\n",
    "        embedding_dim: size of the vocabulary embeddings\n",
    "        hidden_dim: hidden dimensions of the lstm model\n",
    "        output_dim: number of classes in our data\n",
    "        n_layers: number of lstm layers\n",
    "        bidirectional: dummy to specify if this is a bidirectional model\n",
    "        dropout: droptout parameter for the dropout model\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        # return the hidden state and the cell. We will concatenate the last two hidden vectors\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        # apply dropout before passing to fully connected layer\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        \n",
    "        #define output\n",
    "        output=self.fc(hidden)\n",
    "        \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters for the model and instatiate the model class\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pretrained_embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the embedding weights to the model and move the model to the gpu\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss and optimizer\n",
    "#loss_function = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "#nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.optim as optim\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# define our loss function and the parameters for updating the model.\n",
    "# TALK ABOUT BETAS\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.7, 0.99))\n",
    "n_tokens = pretrained_embeddings.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(nn.CrossEntropyLoss)\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.NLLLoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def accuracy(out, labels):\n",
    "#    return torch.sum(labels.data == out)/float(labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_i, batch in enumerate(iterator):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch.text = batch.text.cuda()\n",
    "        batch.product_id=batch.product_id.cuda()\n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "        \n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "        #loss = criterion(predictions, batch.product_id)\n",
    "        #if predictions[0].shape==batch.product_id.shape[0]:\n",
    "        w=weights.cuda    \n",
    "    \n",
    "        loss=criterion(predictions, batch.product_id, weights=w)\n",
    "        \n",
    "        \n",
    "        #acc = binary_accuracy(predictions, batch.product_id)\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        correct = (torch.max(predictions, 1)[1] == batch.product_id).sum()\n",
    "        #print(float(correct)/32)\n",
    "        if batch_i % 10 == 9:    # print every 10 batches\n",
    "            print('Epoch: {}, Batch: {}, Avg. Loss: {}, correct{}'.format(epoch + 1,\n",
    "                                                                          batch_i+1, running_loss/1000,\n",
    "                                                                          float(correct)/32))        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(train_iter)))\n",
    "        #epoch_loss += loss.item()\n",
    "        \n",
    "        #correct += (predicted.type(torch.DoubleTensor) == labels).sum()\n",
    "        #pred=predictions[0]\n",
    "        #print(torch.max(predictions, 0)[1],batch.product_id)\n",
    "        #print(torch.max(predictions, 1)[1].shape)\n",
    "        #correct = (rounded_preds == y).float()\n",
    "        \n",
    "        #correct = (output == batch.product_id).float().sum()\n",
    "        #correct/batch.product_id.shape[0]\n",
    "        #epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test function to measure accuracy\n",
    "\n",
    "def test_data():\n",
    "    \n",
    "    # iterate through the test dataset\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        \n",
    "        # move inputs to gpu\n",
    "        batch.text = batch.text.cuda()\n",
    "        batch.product_id=batch.product_id.cuda()\n",
    "        \n",
    "        predictions = model(batch.text)\n",
    "        \n",
    "        loss=criterion(predictions, batch.product_id)\n",
    "                \n",
    "        #acc = binary_accuracy(predictions, batch.product_id)\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        correct = (torch.max(predictions, 1)[1] == batch.product_id).sum()\n",
    "        #print(float(correct)/32)\n",
    "        if batch_i % 10 == 9:    # print every 10 batches\n",
    "            print('Epoch: {}, Batch: {}, Avg. Loss: {}, correct{}'.format(epoch + 1,\n",
    "                                                                          batch_i+1, running_loss/1000,\n",
    "                                                                          float(correct)/32))        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    #train_loss, train_acc = \n",
    "    train(model, train_iter, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "## TODO: change the name to something uniqe for each new model\n",
    "model_dir = os.path.join(DATA_DIR, 'savedmodels/')\n",
    "model_name = 'simple_lstm_financial_data.pt'\n",
    "\n",
    "# after training, save your model parameters in the dir 'saved_models'\n",
    "torch.save(model.state_dict(), model_dir+model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.functional.softmax(pr1[0,:])\n",
    "print(b1)\n",
    "print(torch.max(torch.nn.functional.softmax(pr1), 1)[1])\n",
    "#(np.log(pr1[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model once its done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
