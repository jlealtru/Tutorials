{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic text preprocessing\n",
    "\n",
    "We will leverage Spacy's modern text preprocessing methods to lemmatize, handle some spelling errors, create a pronoun flag, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify device type\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default data dir\n",
    "basepath = '/home/datawrestler/data/quora/dataset'\n",
    "\n",
    "DATA_DIR = '/home/datawrestler/data/quora'\n",
    "\n",
    "os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'question_text': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.iloc[0:700000].to_csv(os.path.join(basepath, 'train.csv'), index=False)\n",
    "df.iloc[700000:800000].to_csv(os.path.join(basepath, 'test.csv'), index=False)\n",
    "df.iloc[800000:900000].to_csv(os.path.join(basepath, 'valid.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_text = df.iloc[0:400000]['text'].tolist()\n",
    "out_test_text = df.iloc[400000:500000]['text'].tolist()\n",
    "out_valid_text = df.iloc[500000:600000]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the language modelling, we are going to write out to a text file\n",
    "# that is newline separated\n",
    "\n",
    "def write_text_data(fname, text):\n",
    "\n",
    "    with open(os.path.join(basepath, fname), 'w') as outfile:\n",
    "        for line in text:\n",
    "            outfile.write('\\n{}'.format(line))\n",
    "        outfile.close()\n",
    "        \n",
    "write_text_data('train.txt', out_text)\n",
    "write_text_data('test.txt', out_test_text)\n",
    "write_text_data('valid.txt', out_valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_count = 0\n",
    "\n",
    "# use custom tokenizer with large spacy model\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    global tokenize_count\n",
    "    if tokenize_count % 1000 == 0:\n",
    "        sys.stdout.write('\\rDoc: {}'.format(tokenize_count))\n",
    "        sys.stdout.flush()\n",
    "    tokenize_count += 1\n",
    "    return [tok.text for tok in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "from torchtext import data\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets.language_modeling import LanguageModelingDataset\n",
    "\n",
    "\n",
    "\n",
    "class CustomLMData(LanguageModelingDataset):\n",
    "    \n",
    "    name = 'lm_dataset'\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, root=None, train='lmdata.txt',\n",
    "               validation=None, test=None,\n",
    "               **kwargs):\n",
    "        \"\"\"\n",
    "        Create dataset from custom data persisted to disc. Data\n",
    "        must be newline separated text files and path must be designated. \n",
    "        \n",
    "        Arguments:\n",
    "            text_field: The field that will be used for text data.\n",
    "            root: The root directory that the dataset's zip archive will be\n",
    "                expanded into; therefore the directory in whose wikitext-2\n",
    "                subdirectory the data files will be stored.\n",
    "            train: The filename of the train data. Default: 'wiki.train.tokens'.\n",
    "            validation: The filename of the validation data, or None to not\n",
    "                load the validation set. Default: 'wiki.valid.tokens'.\n",
    "            test: The filename of the test data, or None to not load the test\n",
    "                set. Default: 'wiki.test.tokens'.\n",
    "                \n",
    "        Resources: \n",
    "            https://github.com/pytorch/text/blob/master/torchtext/data/dataset.py\n",
    "            https://github.com/pytorch/text/blob/master/torchtext/datasets/language_modeling.py\n",
    "            https://torchtext.readthedocs.io/en/latest/examples.html\n",
    "        \"\"\"\n",
    "        return super(CustomLMData, cls).splits(\n",
    "            root=root, train=train, validation=validation, test=test,\n",
    "            text_field=text_field, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def iters(cls, batch_size=32, bptt_len=35, device=0, path=basepath,\n",
    "              train='lmdata.txt', validation=None, test=None, root=basepath,\n",
    "              vectors=None, **kwargs):\n",
    "        \"\"\"Create iterator objects for splits of the WikiText-2 dataset.\n",
    "        This is the simplest way to use the dataset, and assumes common\n",
    "        defaults for field, vocabulary, and iterator parameters.\n",
    "        Arguments:\n",
    "            batch_size: Batch size.\n",
    "            bptt_len: Length of sequences for backpropagation through time.\n",
    "            device: Device to create batches on. Use -1 for CPU and None for\n",
    "                the currently active GPU device.\n",
    "            root: The root directory that the dataset's zip archive will be\n",
    "                expanded into; therefore the directory in whose wikitext-2\n",
    "                subdirectory the data files will be stored.\n",
    "            wv_dir, wv_type, wv_dim: Passed to the Vocab constructor for the\n",
    "                text field. The word vectors are accessible as\n",
    "                train.dataset.fields['text'].vocab.vectors.\n",
    "            Remaining keyword arguments: Passed to the splits method.\n",
    "        \"\"\"\n",
    "        TEXT = data.Field()\n",
    "\n",
    "        train = cls.splits(TEXT, root=root, path=basepath, **kwargs)\n",
    "\n",
    "        TEXT.build_vocab(train, vectors=vectors)\n",
    "\n",
    "        return data.BPTTIterator.splits(train,\n",
    "            batch_size=batch_size, bptt_len=bptt_len,\n",
    "            device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using our custom dataset class that inherits from the languagemodelling dataset of \n",
    "# torchtext, create our train, test, valid splits of quora questions\n",
    "train, test, valid = CustomLMData.splits(\n",
    "    TEXT,\n",
    "    path=basepath,\n",
    "    train='train.txt',\n",
    "    test='test.txt',\n",
    "    validation='valid.txt',\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "# to see available pretrained embedding options, take a peek at the source code:\n",
    "# https://github.com/pytorch/text/blob/master/torchtext/vocab.py\n",
    "TEXT.build_vocab(train, vectors='glove.42B.300d', min_freq=5, \n",
    "                max_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our batch iterator object for training. This will automatically \n",
    "# shift our input text forward t+1 for our target data for the language model \n",
    "# to predict the next word in the sequence\n",
    "train_iter, test_iter, valid_iter = data.BPTTIterator.splits(\n",
    "    (train, test, valid), \n",
    "    batch_size=128, \n",
    "    bptt_len=40, # specifying the sequence length for back prop through time\n",
    "    device=device,\n",
    "    repeat=False, \n",
    "    sort_key=lambda x: len(x.text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerilization occurs\n",
    "b.text[:5, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can peep into the numerilization with\n",
    "TEXT.vocab.itos[1656]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.target[:5, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and Training the language model\n",
    "\n",
    "The goal here is to use the pretrained glove 300 dimensional vectors to hot start our embedding model that will be fine tuned on our actual data. We are going to build an RNN bidirectional language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)  # 2 for bidirection\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial states\n",
    "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device) # 2 for bidirection \n",
    "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)\n",
    "   \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    " \n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, ntoken, ninp,\n",
    "                 nhid, nlayers, bsz,\n",
    "                 dropout=0.5, tie_weights=True):\n",
    "        \"\"\"\n",
    "        Bidirectional language model \n",
    "        \n",
    "        https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py\n",
    "        \"\"\"\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.bsz = bsz\n",
    "        self.tie_weights = tie_weights # TODO: figure out tying weight with bidirectional LSTM\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout, bidirectional=True)\n",
    "        self.decoder = nn.Linear(nhid*2, ntoken) # we need *2 for bidirectional\n",
    "        self.init_weights()\n",
    "        self.hidden = self.init_hidden(bsz) # the input is a batched consecutive corpus\n",
    "                                            # therefore, we retain the hidden state across batches\n",
    "     \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    " \n",
    "    def forward(self, input_data):\n",
    "        emb = self.drop(self.encoder(input_data))\n",
    "        output, self.hidden = self.lstm(emb, self.hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
    " \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        # once again we need x2 for bidirectional LSTM\n",
    "        return (V(weight.new(self.nlayers*2, bsz, self.nhid).zero_().cuda()),\n",
    "                V(weight.new(self.nlayers*2, bsz, self.nhid).zero_()).cuda())\n",
    "  \n",
    "    def reset_history(self):\n",
    "        self.hidden = tuple(V(v.data) for v in self.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to use our pretrained embeddings to init the RNN\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "weight_matrix = TEXT.vocab.vectors\n",
    "model = BiRNN(weight_matrix.size(0), \n",
    "                 weight_matrix.size(1), 300, 4, BATCH_SIZE, \n",
    "             tie_weights=True)\n",
    "\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "# define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.7, 0.99))\n",
    "n_tokens = weight_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the evaluation criteria\n",
    "\n",
    "def validation_loss(valid_iter, model):\n",
    "    \n",
    "    # monitor the loss\n",
    "    val_loss = 0\n",
    "    # turn on evaluation mode\n",
    "    model.eval()\n",
    "    for batch in valid_iter:\n",
    "        model.reset_history()\n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        val_loss += loss.item() * text.size(0)\n",
    "    val_loss /= len(valid.examples[0].text)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/\n",
    "\n",
    "# and write our training loop\n",
    "\n",
    "\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def clip_grads(model, clip_weight=0.25):\n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_weight)\n",
    "    for p in model.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "    \n",
    "\n",
    "def train_model(num_epochs=10):\n",
    "    \"\"\"One epoch of a training loop\"\"\"\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "        # turn on training mode\n",
    "        epoch_loss = 0\n",
    "        t = tqdm(train_iter)\n",
    "        batch_ii = 0\n",
    "        for batch in t:\n",
    "            batch_ii += 1\n",
    "            # reset the hidden state or else the model will try to backpropagate to the\n",
    "            # beginning of the dataset, requiring lots of time and a lot of memory\n",
    "            model.train()\n",
    "            t.set_description('Epoch: {}'.format(epoch))\n",
    "            t.refresh()\n",
    "            model.reset_history()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text, targets = batch.text, batch.target\n",
    "            prediction = model(text)\n",
    "            # pytorch currently only supports cross entropy loss for inputs of 2 or 4 dimensions.\n",
    "            # we therefore flatten the predictions out across the batch axis so that it becomes\n",
    "            # shape (batch_size * sequence_length, n_tokens)\n",
    "            # in accordance to this, we reshape the targets to be\n",
    "            # shape (batch_size * sequence_length)\n",
    "            loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip gradients\n",
    "            clip_grads(model)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # epoch_loss += loss.data[0] * prediction.size(0) * prediction.size(1)\n",
    "            epoch_loss += loss.item() * prediction.size(0) * prediction.size(1)\n",
    "\n",
    "            epoch_loss /= len(train.examples[0].text)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # print('Epoch: {}, Training Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "        # capture validation loss for each batch\n",
    "        valid_loss = validation_loss(valid_iter, model)\n",
    "        print('Epoch: {} | Training Loss: {:.4f} | Valid Loss: {:.4f}'.format(epoch, \n",
    "                                                                             epoch_loss, \n",
    "                                                                             valid_loss))\n",
    " \n",
    "    final_val_loss = validation_loss(valid_iter, model)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, \n",
    "                                                                             epoch_loss, \n",
    "                                                                             final_val_loss))\n",
    "\n",
    "    \n",
    "train_model(num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "if not os.path.exists(os.path.join(basepath, 'models')):\n",
    "    os.makedirs(os.path.join(basepath, 'models'))\n",
    "    \n",
    "# save entire model - if only wanting to save for inference, \n",
    "# use model.state_dict()\n",
    "torch.save(model, os.path.join(basepath, 'models/lm_300_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.load(os.path.join(basepath, 'models/lm_300_model.pt'))\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ids_to_sentence(id_tensor, vocab, join=None):\n",
    "    \"\"\"Converts a sequence of word ids to a sentence\"\"\"\n",
    "    if isinstance(id_tensor, torch.LongTensor):\n",
    "        ids = id_tensor.transpose(0, 1).contiguous().view(-1)\n",
    "    elif isinstance(id_tensor, np.ndarray):\n",
    "        ids = id_tensor.transpose().reshape(-1)\n",
    "    batch = [vocab.itos[ind] for ind in ids] # denumericalize\n",
    "    if join is None:\n",
    "        return batch\n",
    "    else:\n",
    "        return join.join(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "arrs = model(b.text).cpu().data.numpy()\n",
    "word_ids_to_sentence(np.argmax(arrs, axis=2), TEXT.vocab, join=' ')[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = TEXT.vocab\n",
    "\n",
    "vocab.stoi['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab.__dict__['freqs'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out vocab items\n",
    "\n",
    "wrd_to_embedding = {}\n",
    "for wrd in list(vocab.__dict__['freqs'].keys()):\n",
    "    print(wrd)\n",
    "    lookup_tensor = torch.tensor([vocab.stoi[wrd]], dtype=torch.long, device=device)\n",
    "    emb = model.drop(model.encoder(lookup_tensor))\n",
    "    # convert embedding to numpy array\n",
    "    emb = emb.cpu()\n",
    "    np_array = emb.detach().numpy()\n",
    "    wrd_to_embedding[wrd] = np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "cosine(wrd_to_embedding['successful'], wrd_to_embedding['pick'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(wrd_to_embedding['soldier'], wrd_to_embedding['war'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if vectors actually drifted by looking at our original vectors \n",
    "# from the glove implementation\n",
    "old_w2v = {}\n",
    "\n",
    "for wrd in list(vocab.__dict__['freqs'].keys()):\n",
    "    print(wrd)\n",
    "    wrd_id = vocab.stoi[wrd]\n",
    "    vocab.vectors[wrd_id].cpu().detach().numpy()\n",
    "    old_w2v[wrd] = np_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(wrd_to_embedding['war'], old_w2v['war'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
