{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "import spacy\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up spacy\n",
    "nlp = spacy.load('en_core_web_lg', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.consumerfinance.gov/data-research/hmda/\n",
    "\n",
    "base_path = '/home/datawrestler/data/financial'\n",
    "fname = 'financial.csv'\n",
    "full_path = os.path.join(base_path, fname)\n",
    "\n",
    "df = pd.read_csv(full_path, low_memory=False)\n",
    "# shuffle the inputs\n",
    "df = df.sample(n=df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_complaints = df.loc[df['Consumer complaint narrative'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_complaints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sense for the totla number of possible complaint issues\n",
    "consumer_complaints.groupby('Product')['Complaint ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sense for how long these narratives are\n",
    "consumer_complaints['wrdCount'] = consumer_complaints['Consumer complaint narrative'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are very long narratives - lets split them on paragraphs and align with \n",
    "# doc id so we have a unique docid for each paragraph that can resolve back to \n",
    "# the original docid\n",
    "consumer_complaints['wrdCount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_paragraph(text):\n",
    "    text = text.replace('\\n\\n', '\\n')\n",
    "    text = text.split('\\n')\n",
    "    return text\n",
    "\n",
    "consumer_complaints['paragraphs'] = (consumer_complaints['Consumer complaint narrative']\n",
    "                                     .apply(lambda x: make_paragraph(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split out so we have one row per paragraph\n",
    "# expand out topics to one topic per row\n",
    "tmp = (consumer_complaints.set_index('Complaint ID')['paragraphs']\n",
    "       .apply(pd.Series)\n",
    "       .stack()\n",
    "       .reset_index()\n",
    "       .drop('level_1', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp['index'] = 1\n",
    "tmp['docid'] = tmp.assign(index=1).groupby('Complaint ID')['index'].transform('cumsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop index\n",
    "tmp = tmp.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename \n",
    "tmp = tmp.rename(columns={0: 'complaint'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the cumulative sum of the index with the doc id to create a unique index\n",
    "# based on the paragraph\n",
    "tmp['docid'] = tmp.apply(lambda x: '{}_{}'.format(x['Complaint ID'], x['docid']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tmp['complaint'].tolist()\n",
    "docids = tmp['docid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(docs) == len(set(docids)), \"\"\"docids not unique\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take subset\n",
    "max_ids = 20000\n",
    "docs = docs[0:max_ids]\n",
    "docids = docids[0:max_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_complaints(text):\n",
    "    text = text.replace('\\n', '')\n",
    "    text = strip_multiple_whitespaces(text)\n",
    "    return text\n",
    "\n",
    "docs = [clean_complaints(complaint) for complaint in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# specify device type\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sentences for ELMO\n",
    "paragraphs = []\n",
    "\n",
    "\n",
    "for ii, doc in enumerate(nlp.pipe(docs, batch_size=10000, n_threads=12)):\n",
    "    docid = docids[ii]\n",
    "    sys.stdout.write('\\rIndex: {}'.format(ii))\n",
    "    sys.stdout.flush()\n",
    "    tokens = [tok.text for tok in doc]\n",
    "    paragraphs.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        chunk = l[i:i + n]\n",
    "        docids = [x[0] for x in chunk]\n",
    "        sentences = [x[1] for x in chunk]\n",
    "        yield docids, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large weights file\n",
    "\n",
    "large_options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json'\n",
    "\n",
    "large_weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file\n",
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "# preliminary weights file\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder(options_file=options_file, weight_file=weight_file, cuda_device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = chunks()\n",
    "sents = [sent[1] for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doclen = [len(doc) for doc in docs]\n",
    "np.mean(doclen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim docs\n",
    "max_doc_len = 150\n",
    "docs = [doc[0:max_doc_len] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = elmo.embed_sentences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_vector = []\n",
    "\n",
    "for vec in vectors:\n",
    "    paragraph_vector.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf = pd.DataFrame({'sents': docs, \n",
    "                      'docid': docids,\n",
    "                      'embedding': paragraph_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need a word to vector lookup mapping for out topic model. However, topic modelling \n",
    "# is very sensitive to the word types. We will need better preprocessing than what was used for \n",
    "# ELMO - however, we need to keep track of the index in the sentence for the word when we drop \n",
    "# punctuation, stopwords, etc. \n",
    "\n",
    "def norm_text(input_sentence):\n",
    "    # input sentence is currently tokenized\n",
    "    # input_sentence = ' '.join(input_sentence)\n",
    "    # convert to spacy doc\n",
    "    doc = nlp(input_sentence)\n",
    "    return doc\n",
    "\n",
    "sentdf['spacyDoc'] = sentdf['sents'].apply(lambda x: norm_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf['sents'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf['spacyDoc'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(spacy_doc):\n",
    "    return [token.lemma_ for token in spacy_doc]\n",
    "\n",
    "sentdf['lemmas'] = sentdf['spacyDoc'].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def lemma_to_vec(row):\n",
    "    lemmas = row['lemmas']\n",
    "    vector = row['embedding']\n",
    "    \n",
    "    if len(lemmas) != vector.shape[1]:\n",
    "        return None\n",
    "    \n",
    "    lemma2vec = defaultdict(lambda: [])\n",
    "    \n",
    "    for idx, lemma in enumerate(lemmas):\n",
    "        lemma2vec[lemma].append(vector[0][idx]) # we want to embedding layer - could take the average of all layers\n",
    "    \n",
    "    # finally iterate back over the keys and take the average of each lemmas vector\n",
    "    # i.e. the same word appears multiple times\n",
    "    for key in lemma2vec.keys():\n",
    "        lemma2vec[key] = np.mean(np.array(lemma2vec[key]), axis=0)\n",
    "        \n",
    "    return lemma2vec\n",
    "        \n",
    "sentdf['lemma2vec'] = sentdf.apply(lambda row: lemma_to_vec(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topic Modelling\n",
    "\n",
    "Now we are ready to perform topic modelling. We will do one final pass to extract just the terms we are interested \n",
    "in processing, removing punctuation, etc. and feed into gensim ldamulticore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download smart stopwords list\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "smart = 'http://www.lextek.com/manuals/onix/stopwords2.html'\n",
    "\n",
    "soup = BeautifulSoup(requests.get(smart).content, 'html.parser')\n",
    "\n",
    "pre = soup.find('pre').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartwords = [line for line in pre.split('\\n') if not line.startswith('#') and line != '']\n",
    "smartwords = [token.lemma_ for line in smartwords for token in nlp(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def tm_text(doc):\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ != '-PRON-' and token.text not in string.punctuation and token.pos_ in ['NOUN', 'VERB']]\n",
    "    return tokens\n",
    "\n",
    "sentdf['tm_tokens'] = sentdf['spacyDoc'].apply(lambda x: tm_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter low wrdcount rows\n",
    "sentdf['wrdCount'] = sentdf['tm_tokens'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high word count\n",
    "sentdf_long = sentdf.loc[sentdf['wrdCount'] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "id2word = Dictionary(sentdf_long['tm_tokens'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word.filter_extremes(no_below=2, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in sentdf_long['tm_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(num_topics=12, id2word=id2word, corpus=corpus, passes=10, minimum_probability=0.15, \n",
    "                  per_word_topics=True, minimum_phi_value=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics(num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sentdf_long['tm_tokens'].values[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopics, words, phi = lda.get_document_topics(id2word.doc2bow(text), per_word_topics=True, \n",
    "                                            minimum_phi_value=0.15, minimum_probability=0.15, \n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_doc_topics(text):\n",
    "    \n",
    "    doctopics, words, phi = lda.get_document_topics(id2word.doc2bow(text), per_word_topics=True, \n",
    "                                            minimum_phi_value=0.15, minimum_probability=0.15, \n",
    "                                            )\n",
    "\n",
    "    topicwords = defaultdict(lambda: [])\n",
    "\n",
    "    doctopicnums = [topic[0] for topic in doctopics]\n",
    "\n",
    "    for wrd in words:\n",
    "        wrdid = wrd[0]\n",
    "        topics = wrd[1]\n",
    "        for topicnum in doctopicnums:\n",
    "\n",
    "            if topicnum in topics:\n",
    "                topicwords[topicnum].append(lda.id2word[wrdid])\n",
    "    return doctopics, topicwords\n",
    "    \n",
    "sentdf_long['topics'] = sentdf_long['tm_tokens'].apply(lambda x: return_doc_topics(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf_long['topicnum'] = sentdf_long['topics'].apply(lambda x: x[0])\n",
    "sentdf_long['topics2words'] = sentdf_long['topics'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf_long['topicnum'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a unique doc and sentence id\n",
    "sentdf_long['sentid'] = sentdf_long.index\n",
    "\n",
    "sentdf_long['docid'] = sentdf_long.apply(lambda x: '{}_{}'.format(x['docid'], x['sentid']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand out topics to one topic per row\n",
    "tmp = (sentdf_long.set_index('docid')['topicnum']\n",
    "       .apply(pd.Series)\n",
    "       .stack()\n",
    "       .reset_index()\n",
    "       .drop('level_1', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['topicnum'] = tmp[0].apply(lambda x: x[0])\n",
    "tmp['topicprob'] = tmp[0].apply(lambda x: x[1])\n",
    "tmp = tmp.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf_long['docid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf_long = sentdf_long.drop(['topics', 'topicnum'], axis=1)\n",
    "\n",
    "sentdf_long = pd.merge(sentdf_long, tmp, on='docid')\n",
    "assert sentdf_long.shape[0] == tmp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter topics2words to topic of row\n",
    "sentdf_long['topics2words'] = sentdf_long.apply(lambda x: x['topics2words'][x['topicnum']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf_long['topics2words'].values[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf_long.loc[sentdf_long['lemma2vec'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get average embedding for topic words\n",
    "\n",
    "def average_topic_embedding(row):\n",
    "    embs = row['lemma2vec']\n",
    "    topicwords = row['topics2words']\n",
    "    all_embs = []\n",
    "    for wrd in topicwords:\n",
    "        all_embs.append(embs[wrd])\n",
    "    return np.mean(np.array(all_embs), axis=0)\n",
    "\n",
    "sentdf_long = sentdf_long.loc[sentdf_long['lemma2vec'].notnull()]\n",
    "sentdf_long['topicvector'] = sentdf_long.apply(lambda row: average_topic_embedding(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics(num_topics=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdf_long.groupby('topicnum')['docid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate to topic 4\n",
    "topic4 = sentdf_long.loc[sentdf_long['topicnum'] == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X = topic4['topicvector'].tolist()\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.1, min_samples=10).fit(X)\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic4['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic4.groupby('cluster')['docid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic4.loc[topic4['cluster'] == -1, 'sents'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic4.loc[topic4['cluster'] == 1, 'sents'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = topic4.loc[topic4['cluster'] != -1]\n",
    "X_sne = cluster['topicvector'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "X_embedded = TSNE(n_components=2, init='pca').fit_transform(X_sne)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "# print __version__ # requires version >= 1.9.0\n",
    "\n",
    "\n",
    "def matplotlib_to_plotly(cmap, pl_entries):\n",
    "    h = 1.0/(pl_entries-1)\n",
    "    pl_colorscale = []\n",
    "    \n",
    "    for k in range(pl_entries):\n",
    "        C = list(map(np.uint8, np.array(cmap(k*h)[:3])*255))\n",
    "        pl_colorscale.append([k*h, 'rgb'+str((C[0], C[1], C[2]))])\n",
    "        \n",
    "    return pl_colorscale\n",
    "\n",
    "cmap = matplotlib_to_plotly(plt.cm.rainbow, 4)\n",
    "\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_data = X_embedded.T\n",
    "\n",
    "trace = go.Scatter(x=tsne_data[0], y=tsne_data[1], \n",
    "                   mode='markers', \n",
    "                   marker=dict(# color=colors, \n",
    "                               colorscale=cmap,\n",
    "                               showscale=False,\n",
    "                               line=dict(color='black', width=1)), \n",
    "                  text=cluster['sents'].tolist())\n",
    "\n",
    "iplot([trace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "\n",
    "X = topic4['topicvector'].tolist()\n",
    "# #############################################################################\n",
    "# Compute Affinity Propagation\n",
    "af = AffinityPropagation(preference=-50).fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic4['clusternum'] = labels\n",
    "\n",
    "topic4.groupby('clusternum')['docid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute two different representation for each token.\n",
    "# Each representation is a linear weighted combination for the\n",
    "# 3 layers in ELMo (i.e., charcnn, the outputs of the two BiLSTM))\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n",
    "# move to GPU\n",
    "elmo = elmo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# get sentence lengths\n",
    "sent_len = [len(x[1]) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(character_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 30\n",
    "\n",
    "\n",
    "all_embeddings = []\n",
    "all_sentences = []\n",
    "all_docids = []\n",
    "for ii, chunk in enumerate(chunks(sentences, 8)):\n",
    "    print(ii)\n",
    "    ids = chunk[0]\n",
    "    sents = chunk[1]\n",
    "    # truncate the sentence to prevent gpu memory issues\n",
    "    sents = [sent[0:min(MAX_SENT_LEN, len(sents))] for sent in sents]\n",
    "    all_sentences.append(sents)\n",
    "    all_docids.append(ids)\n",
    "    character_ids = batch_to_ids(sents)\n",
    "    # move to GPU\n",
    "    character_ids = character_ids.to(device)\n",
    "    embeddings = elmo(character_ids)\n",
    "    all_embeddings.append(embeddings)\n",
    "    del character_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = elmo(character_ids)\n",
    "\n",
    "# The first layer corresponds to the context insensitive token representation, \n",
    "# followed by the two LSTM layers. See the ELMo paper or follow up work at EMNLP 2018 \n",
    "# for a description of what types of information is captured in each layer.\n",
    "\n",
    "# embeddings['elmo_representations'] is length two list of tensors.\n",
    "# Each element contains one layer of ELMo representations with shape\n",
    "# (2, 3, 1024).\n",
    "#   2    - the batch size\n",
    "#   3    - the sequence length of the batch\n",
    "#   1024 - the length of each ELMo vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = chunks(sentences, 128)\n",
    "chunk = next(chunker)\n",
    "ids = chunk[0]\n",
    "sents = chunk[1]\n",
    "elmo.eval()\n",
    "character_ids = batch_to_ids(sents)\n",
    "embeddings = elmo(character_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo.train(character_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['elmo_representations'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(elmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from time import sleep\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def clip_grads(model, clip_weight=0.25):\n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip_weight)\n",
    "    for p in model.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "    \n",
    "\n",
    "def train_model(num_epochs=10):\n",
    "    \"\"\"One epoch of a training loop\"\"\"\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "        # turn on training mode\n",
    "        epoch_loss = 0\n",
    "        t = tqdm(train_iter)\n",
    "        batch_ii = 0\n",
    "        for batch in t:\n",
    "            batch_ii += 1\n",
    "            # reset the hidden state or else the model will try to backpropagate to the\n",
    "            # beginning of the dataset, requiring lots of time and a lot of memory\n",
    "            elmo.train()\n",
    "            t.set_description('Epoch: {}'.format(epoch))\n",
    "            t.refresh()\n",
    "             #elmo.reset_history()\n",
    "\n",
    "            elmo.zero_grad()\n",
    "\n",
    "            text, targets = batch.text, batch.target\n",
    "            prediction = model(text)\n",
    "            # pytorch currently only supports cross entropy loss for inputs of 2 or 4 dimensions.\n",
    "            # we therefore flatten the predictions out across the batch axis so that it becomes\n",
    "            # shape (batch_size * sequence_length, n_tokens)\n",
    "            # in accordance to this, we reshape the targets to be\n",
    "            # shape (batch_size * sequence_length)\n",
    "            loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            # clip gradients\n",
    "            clip_grads(model)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # epoch_loss += loss.data[0] * prediction.size(0) * prediction.size(1)\n",
    "            epoch_loss += loss.item() * prediction.size(0) * prediction.size(1)\n",
    "\n",
    "            epoch_loss /= len(train.examples[0].text)\n",
    "            \n",
    "            \n",
    "            \n",
    "        # print('Epoch: {}, Training Loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "        # capture validation loss for each batch\n",
    "        valid_loss = validation_loss(valid_iter, model)\n",
    "        print('Epoch: {} | Training Loss: {:.4f} | Valid Loss: {:.4f}'.format(epoch, \n",
    "                                                                             epoch_loss, \n",
    "                                                                             valid_loss))\n",
    " \n",
    "    final_val_loss = validation_loss(valid_iter, model)\n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, \n",
    "                                                                             epoch_loss, \n",
    "                                                                             final_val_loss))\n",
    "\n",
    "    \n",
    "train_model(num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning]",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
