{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as lite\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.sql import read_sql\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-07-26 14:34:05--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  2.43MB/s    in 33s     \n",
      "\n",
      "2019-07-26 14:34:38 (2.43 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the imdb data\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " aclImdb\t\t fit_head.pth\r\n",
      " aclImdb_v1.tar.gz\t itos.pkl\r\n",
      "'bert imdb.ipynb'\t models\r\n",
      " database.sqlite\t'pitchfork bert.ipynb'\r\n",
      " data_clas_pitch.pkl\t pitchfork_classification_script.ipynb\r\n",
      " data_lm_pitchfork.pkl\t pitchfork_language_model.ipynb\r\n",
      " fine_tuned_enc.pth\t scrape_npr.ipynb\r\n",
      " fine_tuned.pth\t\t Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the directory paths of the imdb reviews\n",
    "train_neg = 'aclImdb/train/neg'\n",
    "train_pos = 'aclImdb/train/pos'\n",
    "\n",
    "test_neg = 'aclImdb/test/neg'\n",
    "test_pos = 'aclImdb/test/pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of training data is 25,000 and test data is 25,000\n"
     ]
    }
   ],
   "source": [
    "# for another implementation see https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"review\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    \n",
    "    float_dict ={'pos': 1, 'neg': 0}\n",
    "    \n",
    "    for file_path in os.listdir(directory):\n",
    "        with open(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"review\"].append(f.read())\n",
    "            #re.match('.*\\/([^-]*)', file_path.group(1))\n",
    "    data[\"sentiment\"] = re.match('.*\\/([^-]*)', directory).group(1)    \n",
    "    data = pd.DataFrame.from_dict(data)\n",
    "    data[\"sentiment\"]= data[\"sentiment\"].map(float_dict)\n",
    "\n",
    "    return data\n",
    "\n",
    "training_data = pd.concat([load_directory_data(train_neg), load_directory_data(train_pos)])\n",
    "training_data = shuffle(training_data)\n",
    "\n",
    "test_data = pd.concat([load_directory_data(test_neg), load_directory_data(test_pos)])\n",
    "test_data = shuffle(test_data)\n",
    "print(f'Lenght of training data is {len(training_data):,} and test data is {len(test_data):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use an iterator class that we will feed into python for training\n",
    "# Next step is define a class that takes the text and labels, tokenizes the text \n",
    "# using the bert tokenizer, converts tokens to ids, pads the sentences to make sure they are the same\n",
    "# size as the model allows; if they are longer it trims them else it pads them with 0.\n",
    "# finallly feeds themn to the classifier.\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)\n",
    "max_seq_length = 256\n",
    "\n",
    "class Data_Processing(object):\n",
    "  \n",
    "  def __init__(self, text_column, label_column):\n",
    "    \n",
    "    self.text_column = text_column.tolist()\n",
    "    \n",
    "    #self.label_column = pd.Categorical(pd.factorize(label_column)[0])\n",
    "    self.label_column = label_column.tolist()\n",
    "    \n",
    "        \n",
    "  def __getitem__(self,index):\n",
    "    \n",
    "    tokenized_text = tokenizer.tokenize(self.text_column[index])\n",
    "    \n",
    "    # Account for [CLS] and [SEP] with \"- 2\"\n",
    "    \n",
    "    if len(tokenized_text) > max_seq_length - 2:\n",
    "      tokenized_text = tokenized_text[:(max_seq_length - 2)]\n",
    "                \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "    \n",
    "    input_mask = [1] * len(input_ids)\n",
    "    \n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding = [0] * (max_seq_length - len(input_ids))\n",
    "    \n",
    "    input_ids += padding\n",
    "    \n",
    "    input_mask += padding\n",
    "   \n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    \n",
    "\n",
    "    \n",
    "    #print(ids_review)\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "        \n",
    "    labels = self.label_column[index] \n",
    "    \n",
    "    #list_of_labels = [torch.from_numpy(np.array(labels)).squeeze(0)]\n",
    "    list_of_labels = torch.tensor(labels)\n",
    "    \n",
    "    return input_ids, list_of_labels\n",
    "  \n",
    "  def __len__(self):\n",
    "        return len(self.text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "batch_size = 16\n",
    "\n",
    "training_data = Data_Processing(training_data['review'].iloc[0:5000], training_data['sentiment'].iloc[0:5000])\n",
    "\n",
    "test_data =  Data_Processing(test_data['review'].iloc[0:5000], test_data['sentiment'].iloc[0:5000])\n",
    "\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "                   'val': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "                   }\n",
    "\n",
    "dataset_sizes = {'train':len(training_data),\n",
    "                'val':len(test_data)}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "# Taken from Huggin Face implementation available here: \n",
    "\n",
    "class BertForSequenceClassification(nn.Module):\n",
    "    \n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary. Items in the batch should begin with the special \"CLS\" token. (see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "    num_labels = 2\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2, output_attentions = False):\n",
    "      \n",
    "      super(BertForSequenceClassification, self).__init__()\n",
    "      #super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "\n",
    "      #super(BertForSequenceClassification, self).__init__()\n",
    "      #self.output_attentions = output_attentions\n",
    "      self.num_labels = num_labels\n",
    "      self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "      self.dropout= nn.Dropout(config.hidden_dropout_prob)\n",
    "      self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "      nn.init.xavier_normal_(self.classifier.weight)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "      \n",
    "      _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "      \n",
    "      pooled_output = self.dropout(pooled_output)\n",
    "      \n",
    "      logits = self.classifier(pooled_output)\n",
    "\n",
    "      return logits\n",
    "    \n",
    "    def freeze_bert_encoder(self):\n",
    "      for param in self.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "      \n",
    "    def unfreeze_bert_encoder(self):\n",
    "      for param in self.bert.parameters():\n",
    "        \n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:00<00:00, 123199.81B/s]\n",
      "100%|██████████| 440473133/440473133 [02:29<00:00, 2950420.77B/s]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import BertConfig\n",
    "\n",
    "config = BertConfig()\n",
    "\n",
    "model = BertForSequenceClassification(config=config, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score\n",
    "# define the metrics to evaluate\n",
    "def log_metrics(y_pred, y_true):\n",
    "    print('Accuracy:', accuracy_score(y_true,y_pred))\n",
    "    #print('MCC:', matthews_corrcoef(y_true,y_pred))\n",
    "    print('AUC score:', roc_auc_score(y_true, y_pred))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import copy \n",
    "import time\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        accuracy_ = []\n",
    "        epoch_loss1 = 0.0\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "              \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            sentiment_corrects = 0\n",
    "            \n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, label in dataloaders_dict[phase]:\n",
    "                \n",
    "                inputs = inputs.to(device) \n",
    "\n",
    "                label = label.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    #print(inputs)\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    outputs = F.softmax(outputs,dim=1)\n",
    "                    #print('here label ', torch.max(label.float(), 1))\n",
    "                    #print(label.squeeze(1))\n",
    "                    loss = criterion(outputs, label.squeeze(0))\n",
    "                    \n",
    "                    preds1 = F.softmax(outputs,dim=1)\n",
    "                    preds1 = torch.argmax(preds1,dim=1).cpu().data.numpy()\n",
    "                    true1 = np.array(label.squeeze(0).cpu().data.numpy())\n",
    "                    \n",
    "                    accuracy_.append(accuracy_score(preds1,true1))\n",
    "                    \n",
    "                    #print(loss)\n",
    "                    #print(outputs, torch.max(label, 1)[0])\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        #print(print(torch.max(outputs, 1)[1] , label.squeeze(0)))\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() \n",
    "                \n",
    "                epoch_loss1 += outputs.shape[0] * loss.item()\n",
    "\n",
    "                \n",
    "                sentiment_corrects += torch.sum(torch.max(outputs, 1)[1] == label.squeeze(0))\n",
    "                #print(running_loss)\n",
    "\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase] \n",
    "            \n",
    "            accuracy_avg = sum(accuracy_) / dataset_sizes[phase]\n",
    "            \n",
    "            print(f'epoch loss 1 is:{epoch_loss} and alt version is {epoch_loss1}, accuracy {accuracy_avg}')\n",
    "\n",
    "            preds = F.softmax(outputs,dim=1)\n",
    "            preds = torch.argmax(preds,dim=1).cpu().data.numpy()\n",
    "            true = np.array(label.squeeze(0).cpu().data.numpy())\n",
    "            \n",
    "            log_metrics(preds,true)\n",
    "            #print(print(torch.max(outputs, 1)[1] , label.squeeze(0)))\n",
    "            \n",
    "            #sentiment_acc = sentiment_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            #print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            #print('{} sentiment_acc: {:.4f}'.format(\n",
    "            #    phase, sentiment_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('saving with loss of {}'.format(epoch_loss),\n",
    "                      'improved over previous {}'.format(best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'bert_model_test.pth')\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "lrlast = .001\n",
    "lrmain = .00001\n",
    "optim1 = optim.Adam(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "       \n",
    "   ])\n",
    "\n",
    "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7142857142857142"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test1 = [1,1,2,3,1,3,1]\n",
    "np.mean(test1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
