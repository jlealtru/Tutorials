{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.sql import read_sql\n",
    "import os\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory paths where we saved the imdb reviews\n",
    "train_neg = 'datasets/aclImdb/train/neg'\n",
    "train_pos = 'datasets/aclImdb/train/pos'\n",
    "\n",
    "test_neg = 'datasets/aclImdb/test/neg'\n",
    "test_pos = 'datasets/aclImdb/test/pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for another implementation see https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"review\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    \n",
    "    float_dict ={'pos': 1, 'neg': 0}\n",
    "  \n",
    "    for file_path in os.listdir(directory):\n",
    "        with open(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"review\"].append(f.read())\n",
    "    \n",
    "    data[\"sentiment\"] = re.match('.*\\/([^-]*)', directory).group(1)    \n",
    "    data = pd.DataFrame.from_dict(data)\n",
    "    data[\"sentiment\"]= data[\"sentiment\"].map(float_dict)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of training data is 25,000 and test data is 25,000\n"
     ]
    }
   ],
   "source": [
    "# laod training and test data using a shuffle function\n",
    "training_data = pd.concat([load_directory_data(train_neg), load_directory_data(train_pos)])\n",
    "training_data = shuffle(training_data)\n",
    "test_data = pd.concat([load_directory_data(test_neg), load_directory_data(test_pos)])\n",
    "test_data = shuffle(test_data)\n",
    "print(f'Lenght of training data is {len(training_data):,} and test data is {len(test_data):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5494</td>\n",
       "      <td>I would rather have 20 root canals than go thr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3821</td>\n",
       "      <td>what can i say about this film that hasnt alre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  sentiment\n",
       "5494  I would rather have 20 root canals than go thr...          0\n",
       "3821  what can i say about this film that hasnt alre...          1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data was parsed correctly\n",
    "training_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use an iterator class that we will feed into python for training\n",
    "# Next step is define a class that takes the text and labels, tokenizes the text \n",
    "# using the bert tokenizer, converts tokens to ids, pads the sentences to make sure they are the same\n",
    "# size as the model allows; if they are longer it trims them else it pads them with 0.\n",
    "# finallly feeds themn to the classifier.\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_seq_length = 256\n",
    "\n",
    "class Data_Processing(object):\n",
    "    def __init__(self, text_column, label_column):\n",
    "        \n",
    "        # define the text column from the dataframe\n",
    "        self.text_column = text_column.tolist()\n",
    "    \n",
    "        #self.label_column = pd.Categorical(pd.factorize(label_column)[0])\n",
    "    \n",
    "        # define the label column and transform it to list\n",
    "        self.label_column = label_column.tolist()\n",
    "    \n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        tokenized_text = tokenizer.tokenize(self.text_column[index])\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "\n",
    "        # check for the sequence lenght taking into consideration the \n",
    "        # fact that we need to include the SEP special token and the CLS special\n",
    "        # tokens. \n",
    "        if len(tokenized_text) > max_seq_length - 2:\n",
    "            tokenized_text = tokenized_text[0:(max_seq_length - 2)]\n",
    "\n",
    "        # We add the CLS token at the beginning of the tokenized sequence and the \n",
    "        # SEP token at the end.\n",
    "        tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "\n",
    "        # convert the inputs to ids (dict looking)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        # We define the size of the input mask to correspon to the lenght of the inputs\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "\n",
    "\n",
    "        #attention_masks.append(seq_mask) \n",
    "\n",
    "        #input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "\n",
    "        # \n",
    "        input_ids += padding\n",
    "\n",
    "        #input_mask += padding\n",
    "\n",
    "        attention_masks = [1 if x>0 else 0 for x in input_ids] \n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(attention_masks) == max_seq_length\n",
    "\n",
    "        #print(ids_review)\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "\n",
    "        labels = self.label_column[index] \n",
    "\n",
    "        #list_of_labels = [torch.from_numpy(np.array(labels)).squeeze(0)]\n",
    "        list_of_labels = torch.tensor(labels)\n",
    "        attention_masks = torch.tensor(attention_masks)\n",
    "        return input_ids, list_of_labels, attention_masks\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# reference for dataloaders https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "import torch\n",
    "batch_size = 4\n",
    "\n",
    "# create a class to process the traininga and test data\n",
    "training_data = Data_Processing(training_data['review'], training_data['sentiment'])\n",
    "\n",
    "test_data =  Data_Processing(test_data['review'], test_data['sentiment'])\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=10),\n",
    "                    'val': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "                   }\n",
    "\n",
    "dataset_sizes = {'train':len(training_data),\n",
    "                 'val':len(test_data)}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  101, 16524,  2143,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2481,  ...,     0,     0,     0],\n",
       "         [  101,  6548,  3899,  ...,     0,     0,     0],\n",
       "         [  101,  1045,  2481,  ...,  1055,  3145,   102]]),\n",
       " tensor([1, 0, 0, 0]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = next(iter(dataloaders_dict.get('train')))\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "config = BertConfig(num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"pruned_heads\": {},\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, roc_auc_score\n",
    "# define the metrics to evaluate\n",
    "def log_metrics(y_pred, y_true):\n",
    "    print('Accuracy:', accuracy_score(y_true,y_pred))\n",
    "    #print('MCC:', matthews_corrcoef(y_true,y_pred))\n",
    "    print('AUC score:', roc_auc_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level = logging.INFO, filename ='bert_imdb_classifier_512.txt', filemode ='w', \n",
    "                   format = '%(name)s -%(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "from torch.optim import lr_scheduler, AdamW\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "lrmain = 2e-5\n",
    "\n",
    "optim1 = optim.AdamW(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "   ])\n",
    "\n",
    "optimizer_ft = optim1\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "#import torch.nn.functional as F\n",
    "import copy \n",
    "import time\n",
    "number_steps = 16\n",
    "print(number_steps)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=3):\n",
    "    best_eval_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        #zero the model gradients\n",
    "        model.zero_grad()\n",
    "        print(f'starting epoch {epoch+1} out of {num_epochs}')\n",
    "        training_loss = []\n",
    "        training_accuracy = []\n",
    "        val_loss = []\n",
    "        val_accuracy = []\n",
    "        outputs_ = []\n",
    "        labels_ = []\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # track number of batches and number of iterations\n",
    "        counter = 0\n",
    "        iterations = 0\n",
    "\n",
    "        # Iterate over data, feeding inputs, attention masks and labels\n",
    "        model.train()\n",
    "        for i, (inputs, label, attention_mask) in enumerate(dataloaders_dict['train']):\n",
    "            # add a counter that will register how many examples we have fed to the\n",
    "            # model\n",
    "            counter+= batch_size\n",
    "            iterations+=1\n",
    "            # move the sequences, labels and masks to the GPU\n",
    "            inputs = inputs.to(device) \n",
    "            label = label.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            # feed the sequences to the model, specifying the attention mask\n",
    "            loss, outputs = model(inputs, attention_mask=attention_mask, labels=label)\n",
    "            outputs1 = outputs\n",
    "            \n",
    "            # feed the logits returned by the model to the softmax to classify the function\n",
    "            outputs = F.softmax(outputs,dim=1)\n",
    "\n",
    "            # calculate the loss function, squeeze the labels so their shapes are compatible\n",
    "            loss_manual = criterion(outputs, label.squeeze(0))\n",
    "            \n",
    "            # divide the loss by the number of steps\n",
    "            loss_reg = loss / number_steps \n",
    "\n",
    "            #add the loss to the epoch loss\n",
    "            epoch_loss += loss_reg\n",
    "            training_loss.append(loss_reg)\n",
    "\n",
    "            loss.backward()\n",
    "      \n",
    "            # accumulate gradients and update every x batches\n",
    "            if (i+1) % number_steps == 0:\n",
    "                \n",
    "                optimizer.step()                            # Now we can do an optimizer step\n",
    "                model.zero_grad()                           # Reset gradients tensors\n",
    "      \n",
    "            # only present the information \n",
    "            if counter%1000 == 0:\n",
    "                # get the predictions and the true labels out of the GPU\n",
    "                preds1 = torch.argmax(outputs,dim=1).cpu().data.numpy()\n",
    "                true1 = np.array(label.squeeze(0).cpu().data.numpy())\n",
    "         \n",
    "        # get the accurary score\n",
    "                training_accuracy.append(accuracy_score(preds1,true1))\n",
    "            \n",
    "                print(f'current training loss is {epoch_loss/iterations} and accuracy is {np.mean(training_accuracy):,.2%}')\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            counter_val = 0\n",
    "            iterations_val = 0\n",
    "            \n",
    "            for i, (inputs, label, attention_mask) in enumerate(dataloaders_dict['val']):\n",
    "                counter_val += batch_size\n",
    "                iterations_val += 1\n",
    "      \n",
    "                # move the sequences, labels and masks to the GPU\n",
    "                inputs = inputs.to(device) \n",
    "                label = label.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "          \n",
    "              # feed the sequences to the model, specifying the attention mask\n",
    "                loss_eval, outputs = model(inputs, attention_mask=attention_mask, labels = label)\n",
    "\n",
    "              # feed the logits returned by the model to the softmax to classify the function\n",
    "                outputs = F.softmax(outputs,dim=1)\n",
    "                \n",
    "              # calculate the loss function, squeeze the labels so their shapes are compatible\n",
    "                loss_eval_manual = criterion(outputs, label.squeeze(0))\n",
    "                val_loss.append(loss_eval)\n",
    "                \n",
    "                preds1 = torch.argmax(outputs,dim=1).cpu().data.numpy()\n",
    "                true1 = np.array(label.squeeze(0).cpu().data.numpy())\n",
    "      \n",
    "                # get the accurary score\n",
    "                val_accuracy.append(accuracy_score(preds1,true1))\n",
    "\n",
    "                if counter_val % 1000 == 0:\n",
    "                    # get the predictions and the true labels out of the GPU for validation\n",
    "                    \n",
    "                    print(f'current validation loss is {np.sum(val_loss)/iterations_val} and accuracy is {np.mean(val_accuracy):,.2%}')\n",
    "                              \n",
    "        print(f'For epoch {epoch+1} training loss is {np.sum(training_loss)/iterations}, \\\n",
    "        training accuracy is {np.mean(training_accuracy):,.2%}, Validation \\\n",
    "        loss is {np.sum(val_loss)/iterations_val} and validation accuracy is {np.mean(val_accuracy):,.2%}')\n",
    "        eval_acc = np.mean(val_accuracy)\n",
    "        if eval_acc >= best_eval_acc:\n",
    "            best_eval_acc = eval_acc\n",
    "            print(f'saving the model with validation accuracy of {eval_acc:,.2%} ')\n",
    "            torch.save(model.state_dict(), 'bert_imdb_classification_state_dict_512.pth')\n",
    "            torch.save(optimizer_ft.state_dict(), 'bert_imdb_classification_optimiser_512.pth')\n",
    "        else:\n",
    "            print(f'model did not improve')\n",
    "        \n",
    "        logging.info(f'We completed epoch {epoch+1} with a training loss of {np.sum(training_loss)/iterations} \\\n",
    "        a training accuracy of {np.mean(training_accuracy):,.2%}, Validation \\\n",
    "        loss is {np.sum(val_loss)/iterations_val} and validation accuracy is {np.mean(val_accuracy):,.2%}')\n",
    "      \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1 out of 3\n",
      "current training loss is 0.04403327777981758 and accuracy is 25.00%\n",
      "current training loss is 0.04156672582030296 and accuracy is 50.00%\n",
      "current training loss is 0.03717075660824776 and accuracy is 58.33%\n",
      "current training loss is 0.03300055116415024 and accuracy is 68.75%\n",
      "current training loss is 0.030756933614611626 and accuracy is 75.00%\n",
      "current training loss is 0.02848183736205101 and accuracy is 75.00%\n",
      "current training loss is 0.026581810787320137 and accuracy is 78.57%\n",
      "current training loss is 0.02508373372256756 and accuracy is 78.12%\n",
      "current training loss is 0.02409145049750805 and accuracy is 80.56%\n",
      "current training loss is 0.023241417482495308 and accuracy is 77.50%\n",
      "current training loss is 0.022398877888917923 and accuracy is 79.55%\n",
      "current training loss is 0.021794715896248817 and accuracy is 81.25%\n",
      "current training loss is 0.021110640838742256 and accuracy is 80.77%\n",
      "current training loss is 0.020723877474665642 and accuracy is 80.36%\n",
      "current training loss is 0.020252559334039688 and accuracy is 81.67%\n",
      "current training loss is 0.019828548654913902 and accuracy is 82.81%\n",
      "current training loss is 0.019311649724841118 and accuracy is 83.82%\n",
      "current training loss is 0.019015217199921608 and accuracy is 84.72%\n",
      "current training loss is 0.01866769604384899 and accuracy is 85.53%\n",
      "current training loss is 0.01835910975933075 and accuracy is 86.25%\n",
      "current training loss is 0.01810181513428688 and accuracy is 86.90%\n",
      "current training loss is 0.017796382308006287 and accuracy is 87.50%\n",
      "current training loss is 0.017553061246871948 and accuracy is 86.96%\n",
      "current training loss is 0.017293065786361694 and accuracy is 87.50%\n",
      "current training loss is 0.017144955694675446 and accuracy is 87.00%\n",
      "current validation loss is 0.17145709693431854 and accuracy is 93.20%\n",
      "current validation loss is 0.17709584534168243 and accuracy is 93.05%\n",
      "current validation loss is 0.1762489229440689 and accuracy is 93.27%\n",
      "current validation loss is 0.17389224469661713 and accuracy is 93.58%\n",
      "current validation loss is 0.17061270773410797 and accuracy is 93.62%\n",
      "current validation loss is 0.17189434170722961 and accuracy is 93.63%\n",
      "current validation loss is 0.17171646654605865 and accuracy is 93.51%\n",
      "current validation loss is 0.17043659090995789 and accuracy is 93.54%\n",
      "current validation loss is 0.17036257684230804 and accuracy is 93.51%\n",
      "current validation loss is 0.1722816675901413 and accuracy is 93.48%\n",
      "current validation loss is 0.17208410799503326 and accuracy is 93.47%\n",
      "current validation loss is 0.17277856171131134 and accuracy is 93.45%\n",
      "current validation loss is 0.17162100970745087 and accuracy is 93.48%\n",
      "current validation loss is 0.17156659066677094 and accuracy is 93.44%\n",
      "current validation loss is 0.172744482755661 and accuracy is 93.39%\n",
      "current validation loss is 0.17273898422718048 and accuracy is 93.37%\n",
      "current validation loss is 0.17192788422107697 and accuracy is 93.46%\n",
      "current validation loss is 0.17198100686073303 and accuracy is 93.48%\n",
      "current validation loss is 0.17380063235759735 and accuracy is 93.43%\n",
      "current validation loss is 0.17520345747470856 and accuracy is 93.41%\n",
      "current validation loss is 0.17509637773036957 and accuracy is 93.41%\n",
      "current validation loss is 0.1753869205713272 and accuracy is 93.38%\n",
      "current validation loss is 0.17588472366333008 and accuracy is 93.35%\n",
      "current validation loss is 0.1761549413204193 and accuracy is 93.33%\n",
      "current validation loss is 0.17482034862041473 and accuracy is 93.37%\n",
      "For epoch 1 training loss is 0.017144955694675446,         training accuracy is 87.00%, Validation         loss is 0.17482034862041473 and validation accuracy is 93.37%\n",
      "saving the model with validation accuracy of 93.37% \n",
      "starting epoch 2 out of 3\n",
      "current training loss is 0.010212396271526814 and accuracy is 75.00%\n",
      "current training loss is 0.009395856410264969 and accuracy is 87.50%\n",
      "current training loss is 0.009268335066735744 and accuracy is 91.67%\n",
      "current training loss is 0.00915272906422615 and accuracy is 87.50%\n",
      "current training loss is 0.00885711144655943 and accuracy is 90.00%\n",
      "current training loss is 0.008825655095279217 and accuracy is 87.50%\n",
      "current training loss is 0.008915935643017292 and accuracy is 89.29%\n",
      "current training loss is 0.00905583705753088 and accuracy is 90.62%\n",
      "current training loss is 0.00887500774115324 and accuracy is 88.89%\n",
      "current training loss is 0.00876748375594616 and accuracy is 90.00%\n",
      "current training loss is 0.008853795938193798 and accuracy is 90.91%\n",
      "current training loss is 0.008915914222598076 and accuracy is 91.67%\n",
      "current training loss is 0.008975675329566002 and accuracy is 92.31%\n",
      "current training loss is 0.008927133865654469 and accuracy is 92.86%\n",
      "current training loss is 0.009124579839408398 and accuracy is 93.33%\n",
      "current training loss is 0.009113181382417679 and accuracy is 93.75%\n",
      "current training loss is 0.009086010046303272 and accuracy is 94.12%\n",
      "current training loss is 0.009129426442086697 and accuracy is 94.44%\n",
      "current training loss is 0.009152619168162346 and accuracy is 93.42%\n",
      "current training loss is 0.009032201021909714 and accuracy is 93.75%\n",
      "current training loss is 0.00899310689419508 and accuracy is 94.05%\n",
      "current training loss is 0.009041877463459969 and accuracy is 94.32%\n",
      "current training loss is 0.008998016826808453 and accuracy is 94.57%\n",
      "current training loss is 0.009054714813828468 and accuracy is 94.79%\n",
      "current training loss is 0.009128046222031116 and accuracy is 95.00%\n",
      "current validation loss is 0.17755044996738434 and accuracy is 93.30%\n",
      "current validation loss is 0.16762134432792664 and accuracy is 93.50%\n",
      "current validation loss is 0.1700059026479721 and accuracy is 93.57%\n",
      "current validation loss is 0.1706993579864502 and accuracy is 93.38%\n",
      "current validation loss is 0.16908040642738342 and accuracy is 93.36%\n",
      "current validation loss is 0.16712750494480133 and accuracy is 93.53%\n",
      "current validation loss is 0.16386990249156952 and accuracy is 93.64%\n",
      "current validation loss is 0.16449466347694397 and accuracy is 93.69%\n",
      "current validation loss is 0.16318398714065552 and accuracy is 93.70%\n",
      "current validation loss is 0.16310761868953705 and accuracy is 93.68%\n",
      "current validation loss is 0.16564741730690002 and accuracy is 93.52%\n",
      "current validation loss is 0.16693109273910522 and accuracy is 93.47%\n",
      "current validation loss is 0.1692984700202942 and accuracy is 93.46%\n",
      "current validation loss is 0.16859988868236542 and accuracy is 93.50%\n",
      "current validation loss is 0.16977879405021667 and accuracy is 93.48%\n",
      "current validation loss is 0.16990239918231964 and accuracy is 93.50%\n",
      "current validation loss is 0.1692851185798645 and accuracy is 93.48%\n",
      "current validation loss is 0.16821525990962982 and accuracy is 93.54%\n",
      "current validation loss is 0.16720783710479736 and accuracy is 93.56%\n",
      "current validation loss is 0.16843344271183014 and accuracy is 93.52%\n",
      "current validation loss is 0.1675986498594284 and accuracy is 93.54%\n",
      "current validation loss is 0.16852881014347076 and accuracy is 93.51%\n",
      "current validation loss is 0.16811087727546692 and accuracy is 93.51%\n",
      "current validation loss is 0.16872233152389526 and accuracy is 93.48%\n",
      "current validation loss is 0.16849729418754578 and accuracy is 93.50%\n",
      "For epoch 2 training loss is 0.009128046222031116,         training accuracy is 95.00%, Validation         loss is 0.16849729418754578 and validation accuracy is 93.50%\n",
      "saving the model with validation accuracy of 93.50% \n",
      "starting epoch 3 out of 3\n",
      "current training loss is 0.003578202798962593 and accuracy is 100.00%\n",
      "current training loss is 0.00375168165192008 and accuracy is 100.00%\n",
      "current training loss is 0.0039766402915120125 and accuracy is 100.00%\n",
      "current training loss is 0.004291680175811052 and accuracy is 100.00%\n",
      "current training loss is 0.004393399227410555 and accuracy is 100.00%\n",
      "current training loss is 0.004411378875374794 and accuracy is 100.00%\n",
      "current training loss is 0.0044813924469053745 and accuracy is 100.00%\n",
      "current training loss is 0.004635362420231104 and accuracy is 100.00%\n",
      "current training loss is 0.004476417787373066 and accuracy is 100.00%\n",
      "current training loss is 0.0048338621854782104 and accuracy is 100.00%\n",
      "current training loss is 0.004833514802157879 and accuracy is 100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current training loss is 0.004900208208709955 and accuracy is 100.00%\n",
      "current training loss is 0.0049104331992566586 and accuracy is 98.08%\n",
      "current training loss is 0.004861780907958746 and accuracy is 98.21%\n",
      "current training loss is 0.0048882910050451756 and accuracy is 96.67%\n",
      "current training loss is 0.004928573966026306 and accuracy is 95.31%\n",
      "current training loss is 0.004909077193588018 and accuracy is 95.59%\n",
      "current training loss is 0.004930620081722736 and accuracy is 95.83%\n",
      "current training loss is 0.00500698946416378 and accuracy is 96.05%\n",
      "current training loss is 0.005010789725929499 and accuracy is 96.25%\n",
      "current training loss is 0.00502385850995779 and accuracy is 96.43%\n",
      "current training loss is 0.005031574983149767 and accuracy is 95.45%\n",
      "current training loss is 0.0049986992962658405 and accuracy is 95.65%\n",
      "current training loss is 0.004966124892234802 and accuracy is 95.83%\n",
      "current training loss is 0.004941725172102451 and accuracy is 96.00%\n",
      "current validation loss is 0.19018147885799408 and accuracy is 94.10%\n",
      "current validation loss is 0.17486022412776947 and accuracy is 94.30%\n",
      "current validation loss is 0.1834261417388916 and accuracy is 94.27%\n",
      "current validation loss is 0.19665589928627014 and accuracy is 93.90%\n",
      "current validation loss is 0.19940805435180664 and accuracy is 93.86%\n",
      "current validation loss is 0.20020556449890137 and accuracy is 93.87%\n",
      "current validation loss is 0.1947537660598755 and accuracy is 94.04%\n",
      "current validation loss is 0.19613239169120789 and accuracy is 93.91%\n",
      "current validation loss is 0.19833220541477203 and accuracy is 93.91%\n",
      "current validation loss is 0.19935381412506104 and accuracy is 93.81%\n",
      "current validation loss is 0.19748498499393463 and accuracy is 93.87%\n",
      "current validation loss is 0.1958674043416977 and accuracy is 93.92%\n",
      "current validation loss is 0.19616125524044037 and accuracy is 93.93%\n",
      "current validation loss is 0.19598767161369324 and accuracy is 93.89%\n",
      "current validation loss is 0.19390340149402618 and accuracy is 93.96%\n",
      "current validation loss is 0.19200555980205536 and accuracy is 94.01%\n",
      "current validation loss is 0.19303667545318604 and accuracy is 93.96%\n",
      "current validation loss is 0.19245660305023193 and accuracy is 93.95%\n",
      "current validation loss is 0.19283956289291382 and accuracy is 93.94%\n",
      "current validation loss is 0.19339606165885925 and accuracy is 93.93%\n",
      "current validation loss is 0.19502820074558258 and accuracy is 93.86%\n",
      "current validation loss is 0.19491414725780487 and accuracy is 93.90%\n",
      "current validation loss is 0.1960013508796692 and accuracy is 93.89%\n",
      "current validation loss is 0.1961817443370819 and accuracy is 93.89%\n",
      "current validation loss is 0.19622640311717987 and accuracy is 93.90%\n",
      "For epoch 3 training loss is 0.004941725172102451,         training accuracy is 96.00%, Validation         loss is 0.19622640311717987 and validation accuracy is 93.90%\n",
      "saving the model with validation accuracy of 93.90% \n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model_ft1 = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(model_ft1[1][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
